thread-1: Start crawling
WebPage index: 00000
Main Page
WebPage index: 00001
Wikipedia
Wikipedia ( i / ˌ w ɪ k ᵻ ˈ p iː d i ə / or i / ˌ w ɪ k i ˈ p iː d i ə / WIK -i- PEE -dee-ə ) is a free online encyclopedia with the aim to allow anyone to edit articles. [3] Wikipedia is the largest and most popular general reference work on the Internet [4] [5] [6] and is ranked among the ten most popular websites. [7] Wikipedia is owned by the nonprofit Wikimedia Foundation . [8] [9] [10]
Wikipedia was launched on January 15, 2001, by Jimmy Wales and Larry Sanger . [11] Sanger coined its name , [12] [13] a portmanteau of wiki [notes 4] and encyclo pedia . There was only the English language version initially, but it quickly developed similar versions in other languages, which differ in content and in editing practices. With 5,398,833 articles , [notes 5] the English Wikipedia is the largest of the more than 290 Wikipedia encyclopedias. Overall, Wikipedia consists of more than 40 million articles in more than 250 different languages [15] and, as of February 2014 [update] , it had 18 billion page views and nearly 500 million unique visitors each month. [16]
As of March 2017, Wikipedia has about forty thousand high-quality articles known as Featured Articles and Good Articles that cover vital topics. [17] [18] In 2005, Nature published a peer review comparing 42 science articles from Encyclopædia Britannica and Wikipedia, and found that Wikipedia's level of accuracy approached Encyclopædia Britannica ' s. [19] Criticism of Wikipedia includes claims that it exhibits systemic bias , presents a mixture of "truths, half truths, and some falsehoods", [20] and that, in controversial topics, it is subject to manipulation and spin . [21]

History

Nupedia
Other collaborative online encyclopedias were attempted before Wikipedia, but none were so successful. [22]
Wikipedia began as a complementary project for Nupedia , a free online English-language encyclopedia project whose articles were written by experts and reviewed under a formal process. [11] Nupedia was founded on March 9, 2000, under the ownership of Bomis , a web portal company. Its main figures were Jimmy Wales , the CEO of Bomis, and Larry Sanger , editor-in-chief for Nupedia and later Wikipedia. Nupedia was licensed initially under its own Nupedia Open Content License, switching to the GNU Free Documentation License before Wikipedia's founding at the urging of Richard Stallman . [23] Sanger and Wales founded Wikipedia. [24] [25] While Wales is credited with defining the goal of making a publicly editable encyclopedia, [26] [27] Sanger is credited with the strategy of using a wiki to reach that goal. [28] On January 10, 2001, Sanger proposed on the Nupedia mailing list to create a wiki as a "feeder" project for Nupedia. [29]

Launch and early growth
Wikipedia was launched on January 15, 2001, as a single English-language edition at www.wikipedia.com, [30] and announced by Sanger on the Nupedia mailing list. [26] Wikipedia's policy of "neutral point-of-view" [31] was codified in its first months. Otherwise, there were relatively few rules initially and Wikipedia operated independently of Nupedia. [26] Originally, Bomis intended to make Wikipedia a business for profit. [32]
Wikipedia gained early contributors from Nupedia, Slashdot postings, and web search engine indexing. By August 8, 2001, Wikipedia had over 8,000 articles. [33] On September 25, 2001, Wikipedia had over 13,000 articles. [34] By the end of 2001, it had grown to approximately 20,000 articles and 18 language editions. It had reached 26 language editions by late 2002, 46 by the end of 2003, and 161 by the final days of 2004. [35] Nupedia and Wikipedia coexisted until the former's servers were taken down permanently in 2003, and its text was incorporated into Wikipedia. The English Wikipedia passed the mark of two million articles on September 9, 2007, making it the largest encyclopedia ever assembled, surpassing even the 1408 Yongle Encyclopedia , which had held the record for almost 600 years. [36]
Citing fears of commercial advertising and lack of control in Wikipedia, users of the Spanish Wikipedia forked from Wikipedia to create the Enciclopedia Libre in February 2002. [37] These moves encouraged Wales to announce that Wikipedia would not display advertisements, and to change Wikipedia's domain from wikipedia.com to wikipedia.org . [38]
Though the English Wikipedia reached three million articles in August 2009, the growth of the edition, in terms of the numbers of articles and of contributors, appears to have peaked around early 2007. [39] Around 1,800 articles were added daily to the encyclopedia in 2006; by 2013 that average was roughly 800. [40] A team at the Palo Alto Research Center attributed this slowing of growth to the project's increasing exclusivity and resistance to change. [41] Others suggest that the growth is flattening naturally because articles that could be called " low-hanging fruit "—topics that clearly merit an article—have already been created and built up extensively. [42] [43] [44]
In November 2009, a researcher at the Rey Juan Carlos University in Madrid ( Spain ) found that the English Wikipedia had lost 49,000 editors during the first three months of 2009; in comparison, the project lost only 4,900 editors during the same period in 2008. [45] [46] The Wall Street Journal cited the array of rules applied to editing and disputes related to such content among the reasons for this trend. [47] Wales disputed these claims in 2009, denying the decline and questioning the methodology of the study. [48] Two years later, in 2011, Wales acknowledged the presence of a slight decline, noting a decrease from "a little more than 36,000 writers" in June 2010 to 35,800 in June 2011. In the same interview, Wales also claimed the number of editors was "stable and sustainable". [49] A 2013 article titled "The Decline of Wikipedia" in MIT's Technology Review questioned this claim. The article revealed that since 2007, Wikipedia had lost a third of the volunteer editors who update and correct the online encyclopedia and those still there have focused increasingly on minutiae. [50] In July 2012, the Atlantic reported that the number of administrators is also in decline. [51] In the November 25, 2013, issue of New York magazine, Katherine Ward stated "Wikipedia, the sixth-most-used website, is facing an internal crisis". [52]

Milestones
In January 2007, Wikipedia entered for the first time the top-ten list of the most popular websites in the U.S., according to comScore Networks. With 42.9 million unique visitors, Wikipedia was ranked number 9, surpassing the New York Times (#10) and Apple (#11). This marked a significant increase over January 2006, when the rank was number 33, with Wikipedia receiving around 18.3 million unique visitors. [53] As of March 2015 [update] , Wikipedia has rank 5 [7] [54] among websites in terms of popularity according to Alexa Internet . In 2014, it received 8 billion pageviews every month. [55] On February 9, 2014, The New York Times reported that Wikipedia has 18 billion page views and nearly 500 million unique visitors a month, "according to the ratings firm comScore." [16]
On January 18, 2012, the English Wikipedia participated in a series of coordinated protests against two proposed laws in the United States Congress—the Stop Online Piracy Act (SOPA) and the PROTECT IP Act (PIPA)—by blacking out its pages for 24 hours . [56] More than 162 million people viewed the blackout explanation page that temporarily replaced Wikipedia content. [57] [58]
Loveland and Reagle argue that, in process, Wikipedia follows a long tradition of historical encyclopedias that accumulated improvements piecemeal through " stigmergic accumulation". [59] [60]
On January 20, 2014, Subodh Varma reporting for The Economic Times indicated that not only had Wikipedia's growth flattened but that it has "lost nearly 10 per cent of its page-views last year. That's a decline of about 2 billion between December 2012 and December 2013. Its most popular versions are leading the slide: page-views of the English Wikipedia declined by 12 per cent, those of German version slid by 17 per cent and the Japanese version lost 9 per cent." [61] Varma added that, "While Wikipedia's managers think that this could be due to errors in counting, other experts feel that Google's Knowledge Graphs project launched last year may be gobbling up Wikipedia users." [61] When contacted on this matter, Clay Shirky, associate professor at New York University and fellow at Harvard's Berkman Center for Internet and Security indicated that he suspected much of the page view decline was due to Knowledge Graphs, stating, "If you can get your question answered from the search page, you don't need to click [any further]." [61]
By the end of December 2016, Wikipedia was ranked fifth in the most popular websites globally. [62]

Openness
Unlike traditional encyclopedias, Wikipedia follows the procrastination principle [notes 6] [64] regarding the security of its content. [64] It started almost entirely open—anyone could create articles, and any Wikipedia article could be edited by any reader, even those who did not have a Wikipedia account. Modifications to all articles would be published immediately. As a result, any article could contain inaccuracies such as errors, ideological biases, and nonsensical or irrelevant text.

Restrictions
Due to the increasing popularity of Wikipedia, popular editions, including the English version, have introduced editing restrictions in some cases. For instance, on the English Wikipedia and some other language editions, only registered users may create a new article. [65] On the English Wikipedia, among others, some particularly controversial, sensitive and/or vandalism-prone pages have been protected to some degree. [66] [67] A frequently vandalized article can be semi-protected or extended confirmed protected , meaning that only autoconfirmed or extended confirmed editors are able to modify it. [68] A particularly contentious article may be locked so that only administrators are able to make changes. [69]
In certain cases, all editors are allowed to submit modifications, but review is required for some editors, depending on certain conditions. For example, the German Wikipedia maintains "stable versions" of articles, [70] which have passed certain reviews. Following protracted trials and community discussion, the English Wikipedia introduced the "pending changes" system in December 2012. [71] Under this system, new and unregistered users' edits to certain controversial or vandalism-prone articles are reviewed by established users before they are published. [72]

Review of changes
Although changes are not systematically reviewed, the software that powers Wikipedia provides certain tools allowing anyone to review changes made by others. The "History" page of each article links to each revision. [notes 7] [73] On most articles, anyone can undo others' changes by clicking a link on the article's history page. Anyone can view the latest changes to articles, and anyone may maintain a "watchlist" of articles that interest them so they can be notified of any changes. "New pages patrol" is a process whereby newly created articles are checked for obvious problems. [74]
In 2003, economics PhD student Andrea Ciffolilli argued that the low transaction costs of participating in a wiki create a catalyst for collaborative development, and that features such as allowing easy access to past versions of a page favor "creative construction" over "creative destruction". [75]

Vandalism
Any change or edit that manipulates content in a way that purposefully compromises the integrity of Wikipedia is considered vandalism. The most common and obvious types of vandalism include additions of obscenities and crude humor. Vandalism can also include advertising and other types of spam. [76] Sometimes editors commit vandalism by removing content or entirely blanking a given page. Less common types of vandalism, such as the deliberate addition of plausible but false information to an article, can be more difficult to detect. Vandals can introduce irrelevant formatting, modify page semantics such as the page's title or categorization, manipulate the underlying code of an article, or use images disruptively. [77]
Obvious vandalism is generally easy to remove from Wikipedia articles; the median time to detect and fix vandalism is a few minutes. [78] [79] However, some vandalism takes much longer to repair. [80]
In the Seigenthaler biography incident , an anonymous editor introduced false information into the biography of American political figure John Seigenthaler in May 2005. Seigenthaler was falsely presented as a suspect in the assassination of John F. Kennedy. [80] The article remained uncorrected for four months. [80] Seigenthaler, the founding editorial director of USA Today and founder of the Freedom Forum First Amendment Center at Vanderbilt University , called Wikipedia co-founder Jimmy Wales and asked whether he had any way of knowing who contributed the misinformation. Wales replied that he did not, although the perpetrator was eventually traced. [81] [82] After the incident, Seigenthaler described Wikipedia as "a flawed and irresponsible research tool". [80] This incident led to policy changes at Wikipedia, specifically targeted at tightening up the verifiability of biographical articles of living people . [83]

Policies and laws
Content in Wikipedia is subject to the laws (in particular, copyright laws) of the United States and of the U.S. state of Virginia , where the majority of Wikipedia's servers are located. Beyond legal matters, the editorial principles of Wikipedia are embodied in the "five pillars" and in numerous policies and guidelines intended to appropriately shape content. Even these rules are stored in wiki form, and Wikipedia editors write and revise the website's policies and guidelines. [84] Editors can enforce these rules by deleting or modifying non-compliant material. Originally, rules on the non-English editions of Wikipedia were based on a translation of the rules for the English Wikipedia. They have since diverged to some extent. [70]

Content policies and guidelines
According to the rules on the English Wikipedia, each entry in Wikipedia must be about a topic that is encyclopedic and is not a dictionary entry or dictionary-like. [85] A topic should also meet Wikipedia's standards of "notability" , [86] which generally means that the topic must have been covered in mainstream media or major academic journal sources that are independent of the article's subject. Further, Wikipedia intends to convey only knowledge that is already established and recognized. [87] It must not present original research . A claim that is likely to be challenged requires a reference to a reliable source . Among Wikipedia editors, this is often phrased as "verifiability, not truth" to express the idea that the readers, not the encyclopedia, are ultimately responsible for checking the truthfulness of the articles and making their own interpretations. [88] This can at times lead to the removal of information that is valid. [89] Finally, Wikipedia must not take sides. [90] All opinions and viewpoints, if attributable to external sources, must enjoy an appropriate share of coverage within an article. [91] This is known as neutral point of view (NPOV).

Governance
Wikipedia's initial anarchy integrated democratic and hierarchical elements over time. [92] [93] An article is not considered to be owned by its creator or any other editor and is not vetted by any recognized authority. [94] Wikipedia's contributors avoid a tragedy of the commons by internalizing benefits. They do this by experiencing flow and identifying with and gaining status in the Wikipedia community. [95]

Administrators
Editors in good standing in the community can run for one of many levels of volunteer stewardship: this begins with " administrator ", [96] [97] privileged users who can delete pages, prevent articles from being changed in case of vandalism or editorial disputes, and try to prevent certain persons from editing. Despite the name, administrators are not supposed to enjoy any special privilege in decision-making; instead, their powers are mostly limited to making edits that have project-wide effects and thus are disallowed to ordinary editors, and to implement restrictions intended to prevent certain persons from making disruptive edits (such as vandalism). [98] [99]
Fewer editors become administrators than in years past, in part because the process of vetting potential Wikipedia administrators has become more rigorous. [100]
Bureaucrats name new administrators, solely upon the recommendations from the community.

Dispute resolution
Wikipedians often have disputes regarding content, which may result in repeatedly making opposite changes to an article, known as edit warring [101] [102] Over time, Wikipedia has developed a semi-formal dispute resolution process to assist in such circumstances. In order to determine community consensus, editors can raise issues at appropriate community forums, [notes 8] or seek outside input through third opinion requests or by initiating a more general community discussion known as a request for comment .

Arbitration Committee
The Arbitration Committee presides over the ultimate dispute resolution process. Although disputes usually arise from a disagreement between two opposing views on how an article should read, the Arbitration Committee explicitly refuses to directly rule on the specific view that should be adopted. Statistical analyses suggest that the committee ignores the content of disputes and rather focuses on the way disputes are conducted, [103] functioning not so much to resolve disputes and make peace between conflicting editors, but to weed out problematic editors while allowing potentially productive editors back in to participate. Therefore, the committee does not dictate the content of articles, although it sometimes condemns content changes when it deems the new content violates Wikipedia policies (for example, if the new content is considered biased ). Its remedies include cautions and probations (used in 63% of cases) and banning editors from articles (43%), subject matters (23%) or Wikipedia (16%). Complete bans from Wikipedia are generally limited to instances of impersonation and anti-social behavior . When conduct is not impersonation or anti-social, but rather anti-consensus or in violation of editing policies, remedies tend to be limited to warnings. [104]

Community
Each article and each user of Wikipedia has an associated "Talk" page. These form the primary communication channel for editors to discuss, coordinate and debate. [105]
Wikipedia's community has been described as cult -like, [106] although not always with entirely negative connotations. [107] The project's preference for cohesiveness, even if it requires compromise that includes disregard of credentials , has been referred to as " anti-elitism ". [108]
Wikipedians sometimes award one another virtual barnstars for good work. These personalized tokens of appreciation reveal a wide range of valued work extending far beyond simple editing to include social support, administrative actions, and types of articulation work. [109]
Wikipedia does not require that its editors and contributors provide identification. [110] As Wikipedia grew, "Who writes Wikipedia?" became one of the questions frequently asked on the project. [111] Jimmy Wales once argued that only "a community ... a dedicated group of a few hundred volunteers" makes the bulk of contributions to Wikipedia and that the project is therefore "much like any traditional organization". [112] In 2008, a Slate magazine article reported that: "According to researchers in Palo Alto, 1 percent of Wikipedia users are responsible for about half of the site's edits." [113] This method of evaluating contributions was later disputed by Aaron Swartz , who noted that several articles he sampled had large portions of their content (measured by number of characters) contributed by users with low edit counts. [114]
The English Wikipedia has 5,398,833 articles, 30,860,582 registered editors, and 134,491 active editors. An editor is considered active if they have made one or more edits in the past thirty days.
Editors who fail to comply with Wikipedia cultural rituals, such as signing talk page comments, may implicitly signal that they are Wikipedia outsiders, increasing the odds that Wikipedia insiders may target or discount their contributions. Becoming a Wikipedia insider involves non-trivial costs: the contributor is expected to learn Wikipedia-specific technological codes, submit to a sometimes convoluted dispute resolution process, and learn a "baffling culture rich with in-jokes and insider references". [115] Editors who do not log in are in some sense second-class citizens on Wikipedia, [115] as "participants are accredited by members of the wiki community, who have a vested interest in preserving the quality of the work product, on the basis of their ongoing participation", [116] but the contribution histories of anonymous unregistered editors recognized only by their IP addresses cannot be attributed to a particular editor with certainty.
A 2007 study by researchers from Dartmouth College found that "anonymous and infrequent contributors to Wikipedia […] are as reliable a source of knowledge as those contributors who register with the site". [117] Jimmy Wales stated in 2009 that "(I)t turns out over 50% of all the edits are done by just .7% of the users... 524 people... And in fact the most active 2%, which is 1400 people, have done 73.4% of all the edits." [112] However, Business Insider editor and journalist Henry Blodget showed in 2009 that in a random sample of articles, most content in Wikipedia (measured by the amount of contributed text that survives to the latest sampled edit) is created by "outsiders", while most editing and formatting is done by "insiders". [112]
A 2008 study found that Wikipedians were less agreeable, open, and conscientious than others, [118] [119] although a later commentary pointed out serious flaws, including that the data showed higher openness, that the differences with the control group were small as were the samples. [120] According to a 2009 study, there is "evidence of growing resistance from the Wikipedia community to new content". [121]

Diversity
One study found that the contributor base to Wikipedia "was barely 13% women; the average age of a contributor was in the mid-20s". [122] A 2011 study by researchers from the University of Minnesota found that females comprised 16.1% of the 38,497 editors who started editing Wikipedia during 2009. [123] In a January 2011 New York Times article, Noam Cohen observed that just 13% of Wikipedia's contributors are female according to a 2008 Wikimedia Foundation survey. [124] Sue Gardner , a former executive director of the Wikimedia Foundation, hoped to see female contributions increase to 25% by 2015. [125] Linda Basch, president of the National Council for Research on Women, noted the contrast in these Wikipedia editor statistics with the percentage of women currently completing bachelor's degrees, master's degrees and PhD programs in the United States (all at rates of 50 percent or greater). [126]
In response, various universities have hosted edit-a-thons to encourage more women to participate in the Wikipedia community. In fall 2013, 15 colleges and universities — including Yale, Brown, and Pennsylvania State — offered college credit for students to "write feminist thinking" about technology into Wikipedia. [127] Estimates of the diversity of contributors by educational level have indicated that sixty-two percent of Wikipedia's editors are at the high school and undergraduate college level of education. [128]
In August 2014, Wikipedia co-founder Jimmy Wales said in a BBC interview that the Wikimedia Foundation was "... really doubling down our efforts ..." to reach 25% of female editors (originally targeted by 2015), since the Foundation had "totally failed" so far. Wales said "a lot of things need to happen ... a lot of outreach, a lot of software changes". [129] Andrew Lih, writing in The New York Times , was quoted by Bloomberg News in December 2016 as supporting Wales comments concerning shortfalls in Wikipedia's outreach to female editors. Lih states his concern with the question indicating that: "How can you get people to participate in an (editing) environment that feels unsafe, where identifying yourself as a woman, as a feminist, could open you up to ugly, intimidating behavior". [130]

Language editions
There are currently 295 language editions of Wikipedia (also called language versions , or simply Wikipedias ). Thirteen of these have over one million articles each ( English , Swedish , Cebuano , German , Dutch , French , Russian , Italian , Spanish , Waray-Waray , Polish , Vietnamese and Japanese ), five more have over 500,000 articles ( Portuguese , Chinese , Ukrainian , Catalan and Persian ), 40 more have over 100,000 articles, and 77 more have over 10,000 articles. [131] [132] The largest, the English Wikipedia, has over 5.3 million articles. As of January 2017 [update] , according to Alexa, the English subdomain (en.wikipedia.org; English Wikipedia) receives approximately 55% of Wikipedia's cumulative traffic, with the remaining split among the other languages (Russian: 9%; Japanese: 7%; Spanish: 6%; French: 4%). [7] As of May 2017, the six largest language editions are (in order of article count) the English , Cebuano , Swedish , German , Dutch , and French Wikipedias. [133]
The unit for the numbers in bars is articles.
Since Wikipedia is based on the Web and therefore worldwide, contributors to the same language edition may use different dialects or may come from different countries (as is the case for the English edition ). These differences may lead to some conflicts over spelling differences (e.g. colour versus color ) [136] or points of view. [137]
Though the various language editions are held to global policies such as "neutral point of view", they diverge on some points of policy and practice, most notably on whether images that are not licensed freely may be used under a claim of fair use . [138] [139] [140]
Jimmy Wales has described Wikipedia as "an effort to create and distribute a free encyclopedia of the highest possible quality to every single person on the planet in their own language". [141] Though each language edition functions more or less independently, some efforts are made to supervise them all. They are coordinated in part by Meta-Wiki, the Wikimedia Foundation's wiki devoted to maintaining all of its projects (Wikipedia and others). [142] For instance, Meta-Wiki provides important statistics on all language editions of Wikipedia, [143] and it maintains a list of articles every Wikipedia should have. [144] The list concerns basic content by subject: biography, history, geography, society, culture, science, technology, and mathematics. As for the rest, it is not rare for articles strongly related to a particular language not to have counterparts in another edition. For example, articles about small towns in the United States might only be available in English, even when they meet notability criteria of other language Wikipedia projects.
Translated articles represent only a small portion of articles in most editions, in part because fully automated translation of articles is disallowed. [145] Articles available in more than one language may offer " interwiki links ", which link to the counterpart articles in other editions.
A study published by PLOS ONE in 2012 also estimated the share of contributions to different editions of Wikipedia from different regions of the world. It reported that the proportion of the edits made from North America was 51% for the English Wikipedia , and 25% for the simple English Wikipedia . [146] The Wikimedia Foundation hopes to increase the number of editors in the Global South to thirty-seven percent by 2015. [147]
On March 1, 2014, The Economist in an article titled "The Future of Wikipedia" cited a trend analysis concerning data published by Wikimedia stating that: "The number of editors for the English-language version has fallen by a third in seven years." [148] The attrition rate for active editors in English Wikipedia was cited by The Economist as substantially in contrast to statistics for Wikipedia in other languages (non-English Wikipedia). The Economist reported that the number of contributors with an average of five of more edits per month was relatively constant since 2008 for Wikipedia in other languages at approximately 42,000 editors within narrow seasonal variances of about 2,000 editors up or down. The attrition rates for editors in English Wikipedia, by sharp comparison, were cited as peaking in 2007 at approximately 50,000 editors which has dropped to 30,000 editors as of the start of 2014. At the quoted trend rate, the number of active editors in English Wikipedia has lost approximately 20,000 editors to attrition since 2007, and the documented trend rate indicates the loss of another 20,000 editors by 2021, down to 10,000 active editors on English Wikipedia by 2021 if left unabated. [148] Given that the trend analysis published in The Economist presents the number of active editors for Wikipedia in other languages (non-English Wikipedia) as remaining relatively constant and successful in sustaining its numbers at approximately 42,000 active editors, the contrast has pointed to the effectiveness of Wikipedia in other languages to retain its active editors on a renewable and sustained basis. [148] No comment was made concerning which of the differentiated edit policy standards from Wikipedia in other languages (non-English Wikipedia) would provide a possible alternative to English Wikipedia for effectively ameliorating substantial editor attrition rates on the English language Wikipedia. [149]

Critical reception
Several Wikipedians have criticized Wikipedia's large and growing regulation , which includes over 50 policies and nearly 150,000 words as of 2014 [update] . [150] [151]
Critics have stated that Wikipedia exhibits systemic bias . Columnist and journalist Edwin Black criticizes Wikipedia for being a mixture of "truth, half truth, and some falsehoods". [20] Articles in The Chronicle of Higher Education and The Journal of Academic Librarianship have criticized Wikipedia's Undue Weight policy, concluding that the fact that Wikipedia explicitly is not designed to provide correct information about a subject, but rather focus on all the major viewpoints on the subject and give less attention to minor ones, creates omissions that can lead to false beliefs based on incomplete information. [152] [153] [154]
Journalists Oliver Kamm and Edwin Black noted how articles are dominated by the loudest and most persistent voices, usually by a group with an "ax to grind" on the topic. [20] [155] An article in Education Next Journal concluded that as a resource about controversial topics, Wikipedia is notoriously subject to manipulation and spin . [21]
In 2006, the Wikipedia Watch criticism website listed dozens of examples of plagiarism in the English Wikipedia. [156]

Accuracy of content
Articles for traditional encyclopedias such as Encyclopædia Britannica are carefully and deliberately written by experts, lending such encyclopedias a reputation for accuracy. [157] Conversely, Wikipedia is often cited for factual inaccuracies and misrepresentations. However, a peer review in 2005 of forty-two scientific entries on both Wikipedia and Encyclopædia Britannica by the science journal Nature found few differences in accuracy, and concluded that "the average science entry in Wikipedia contained around four inaccuracies; Britannica , about three." [19] Reagle suggested that while the study reflects "a topical strength of Wikipedia contributors" in science articles, "Wikipedia may not have fared so well using a random sampling of articles or on humanities subjects." [158] The findings by Nature were disputed by Encyclopædia Britannica , [159] [160] and in response, Nature gave a rebuttal of the points raised by Britannica . [161] In addition to the point-for-point disagreement between these two parties, others have examined the sample size and selection method used in the Nature effort, and suggested a "flawed study design" (in Nature ' s manual selection of articles, in part or in whole, for comparison), absence of statistical analysis (e.g., of reported confidence intervals ), and a lack of study "statistical power" (i.e., owing to small sample size, 42 or 4 x 10 1 articles compared, vs >10 5 and >10 6 set sizes for Britannica and the English Wikipedia, respectively). [162]
As a consequence of the open structure, Wikipedia "makes no guarantee of validity" of its content, since no one is ultimately responsible for any claims appearing in it. [163] Concerns have been raised by PC World in 2009 regarding the lack of accountability that results from users' anonymity, [164] the insertion of false information, [165] vandalism , and similar problems.
Economist Tyler Cowen wrote: "If I had to guess whether Wikipedia or the median refereed journal article on economics was more likely to be true, after a not so long think I would opt for Wikipedia." He comments that some traditional sources of non-fiction suffer from systemic biases and novel results, in his opinion, are over-reported in journal articles and relevant information is omitted from news reports. However, he also cautions that errors are frequently found on Internet sites, and that academics and experts must be vigilant in correcting them. [166]
Critics argue that Wikipedia's open nature and a lack of proper sources for most of the information makes it unreliable. [167] Some commentators suggest that Wikipedia may be reliable, but that the reliability of any given article is not clear. [168] Editors of traditional reference works such as the Encyclopædia Britannica have questioned the project's utility and status as an encyclopedia. [169]
Wikipedia's open structure inherently makes it an easy target for Internet trolls , spammers , and various forms of paid advocacy seen as counterproductive to the maintenance of a neutral and verifiable online encyclopedia. [73] [171] In response to paid advocacy editing and undisclosed editing issues, Wikipedia was reported in an article by Jeff Elder in The Wall Street Journal on June 16, 2014, to have strengthened its rules and laws against undisclosed editing. [172] The article stated that: "Beginning Monday [from date of article], changes in Wikipedia's terms of use will require anyone paid to edit articles to disclose that arrangement. Katherine Maher , the nonprofit Wikimedia Foundation's chief communications officer, said the changes address a sentiment among volunteer editors that, 'we're not an advertising service; we're an encyclopedia.'" [172] [173] [174] [175] [176] These issues, among others, had been parodied since the first decade of Wikipedia, notably by Stephen Colbert on The Colbert Report . [177]
Most university lecturers discourage students from citing any encyclopedia in academic work , preferring primary sources ; [178] some specifically prohibit Wikipedia citations. [179] [180] Wales stresses that encyclopedias of any type are not usually appropriate to use as citeable sources, and should not be relied upon as authoritative. [181] Wales once (2006 or earlier) said he receives about ten emails weekly from students saying they got failing grades on papers because they cited Wikipedia; he told the students they got what they deserved. "For God's sake, you're in college; don't cite the encyclopedia", he said. [182]
In February 2007, an article in The Harvard Crimson newspaper reported that a few of the professors at Harvard University were including Wikipedia articles in their syllabi , although without realizing the articles might change. [183] In June 2007, former president of the American Library Association Michael Gorman condemned Wikipedia, along with Google , [184] stating that academics who endorse the use of Wikipedia are "the intellectual equivalent of a dietitian who recommends a steady diet of Big Macs with everything".
A Harvard law textbook, Legal Research in a Nutshell (2011), cites Wikipedia as a "general source" that "can be a real boon" in "coming up to speed in the law governing a situation" and, "while not authoritative, can provide basic facts as well as leads to more in-depth resources". [185]

Medical information
On March 5, 2014, Julie Beck writing for The Atlantic magazine in an article titled "Doctors' #1 Source for Healthcare Information: Wikipedia", stated that "Fifty percent of physicians look up conditions on the (Wikipedia) site, and some are editing articles themselves to improve the quality of available information." [186] Beck continued to detail in this article new programs of Dr. Amin Azzam at the University of San Francisco to offer medical school courses to medical students for learning to edit and improve Wikipedia articles on health-related issues , as well as internal quality control programs within Wikipedia organized by Dr. James Heilman to improve a group of 200 health-related articles of central medical importance up to Wikipedia's highest standard of articles using its Featured Article and Good Article peer review evaluation process. [186] In a May 7, 2014, follow-up article in The Atlantic titled "Can Wikipedia Ever Be a Definitive Medical Text?", Julie Beck quotes Wikiproject Medicine's Dr. James Heilman as stating: "Just because a reference is peer-reviewed doesn't mean it's a high-quality reference." [187] Beck added that: "Wikipedia has its own peer review process before articles can be classified as 'good' or 'featured.' Heilman, who has participated in that process before, says 'less than 1 percent' of Wikipedia's medical articles have passed. [187]

Quality of writing
In 2008, researchers at Carnegie Mellon University found that the quality of a Wikipedia article would suffer rather than gain from adding more writers when the article lacked appropriate explicit or implicit coordination. [188] For instance, when contributors rewrite small portions of an entry rather than making full-length revisions, high- and low-quality content may be intermingled within an entry. Roy Rosenzweig , a history professor, stated that American National Biography Online outperformed Wikipedia in terms of its "clear and engaging prose", which, he said, was an important aspect of good historical writing. [189] Contrasting Wikipedia's treatment of Abraham Lincoln to that of Civil War historian James McPherson in American National Biography Online , he said that both were essentially accurate and covered the major episodes in Lincoln's life, but praised "McPherson's richer contextualization […] his artful use of quotations to capture Lincoln's voice […] and […] his ability to convey a profound message in a handful of words." By contrast, he gives an example of Wikipedia's prose that he finds "both verbose and dull". Rosenzweig also criticized the "waffling—encouraged by the NPOV policy—[which] means that it is hard to discern any overall interpretive stance in Wikipedia history". By example, he quoted the conclusion of Wikipedia's article on William Clarke Quantrill . While generally praising the article, he pointed out its "waffling" conclusion: "Some historians […] remember him as an opportunistic, bloodthirsty outlaw, while others continue to view him as a daring soldier and local folk hero." [189]
Other critics have made similar charges that, even if Wikipedia articles are factually accurate, they are often written in a poor, almost unreadable style. Frequent Wikipedia critic Andrew Orlowski commented: "Even when a Wikipedia entry is 100 per cent factually correct, and those facts have been carefully chosen, it all too often reads as if it has been translated from one language to another then into to a third, passing an illiterate translator at each stage." [190] A study of Wikipedia articles on cancer was conducted in 2010 by Yaacov Lawrence of the Kimmel Cancer Center at Thomas Jefferson University . The study was limited to those articles that could be found in the Physician Data Query and excluded those written at the "start" class or "stub" class level. Lawrence found the articles accurate but not very readable, and thought that "Wikipedia's lack of readability (to non-college readers) may reflect its varied origins and haphazard editing". [191] The Economist argued that better-written articles tend to be more reliable: "inelegant or ranting prose usually reflects muddled thoughts and incomplete information". [192]

Coverage of topics and systemic bias
Wikipedia seeks to create a summary of all human knowledge in the form of an online encyclopedia, with each topic covered encyclopedically in one article. Since it has terabytes of disk space, it can have far more topics than can be covered by any printed encyclopedia. [193] The exact degree and manner of coverage on Wikipedia is under constant review by its editors, and disagreements are not uncommon (see deletionism and inclusionism ). [194] [195] Wikipedia contains materials that some people may find objectionable, offensive, or pornographic because Wikipedia is not censored . The policy has sometimes proved controversial: in 2008, Wikipedia rejected an online petition against the inclusion of images of Muhammad in the English edition of its Muhammad article, citing this policy. The presence of politically, religiously, and pornographically sensitive materials in Wikipedia has led to the censorship of Wikipedia by national authorities in China , [196] Pakistan , [197] and the United Kingdom , [198] among other countries.
A 2008 study conducted by researchers at Carnegie Mellon University and Palo Alto Research Center gave a distribution of topics as well as growth (from July 2006 to January 2008) in each field: [199]
These numbers refer only to the quantity of articles: it is possible for one topic to contain a large number of short articles and another to contain a small number of large ones. Through its " Wikipedia Loves Libraries " program, Wikipedia has partnered with major public libraries such as the New York Public Library for the Performing Arts to expand its coverage of underrepresented subjects and articles. [200]
A 2011 study conducted by researchers at the University of Minnesota indicated that male and female editors focus on different coverage topics. There was a greater concentration of females in the People and Arts category, while males focus more on Geography and Science. [201]

Coverage of topics and selection bias
Research conducted by Mark Graham of the Oxford Internet Institute in 2009 indicated that the geographic distribution of article topics is highly uneven. Africa is most underrepresented. [202]
An editorial in The Guardian in 2014 noted that women porn stars are better covered than women writers as a further example. [203]

Systemic bias
When multiple editors contribute to one topic or set of topics, systemic bias may arise, due to the demographic backgrounds of the editors. In 2011, Wales noted that the unevenness of coverage is a reflection of the demography of the editors, which predominantly consists of young males with high education levels in the developed world (cf. previously). [49] The October 22, 2013 essay by Tom Simonite in MIT's Technology Review titled "The Decline of Wikipedia" discussed the effect of systemic bias and policy creep on the downward trend in the number of editors . [50]
Systemic bias on Wikipedia may follow that of culture generally, for example favoring certain nationalities, ethnicities or majority religions. [204] It may more specifically follow the biases of Internet culture , inclining to being young, male, English-speaking, educated, technologically aware, and wealthy enough to spare time for editing. Biases of its own may include over-emphasis on topics such as pop culture, technology, and current events. [204]
Taha Yasseri of the University of Oxford , in 2013, studied the statistical trends of systemic bias at Wikipedia introduced by editing conflicts and their resolution. [205] [206] His research examined the counterproductive work behavior of edit warring. Yasseri contended that simple reverts or "undo" operations were not the most significant measure of counterproductive behavior at Wikipedia and relied instead on the statistical measurement of detecting "reverting/reverted pairs" or "mutually reverting edit pairs". Such a "mutually reverting edit pair" is defined where one editor reverts the edit of another editor who then, in sequence, returns to revert the first editor in the "mutually reverting edit pairs". The results were tabulated for several language versions of Wikipedia. The English Wikipedia's three largest conflict rates belonged to the articles George W. Bush , Anarchism and Muhammad . [206] By comparison, for the German Wikipedia, the three largest conflict rates at the time of the Oxford study were for the articles covering (i) Croatia , (ii) Scientology and (iii) 9/11 conspiracy theories . [206]
Researchers from the Washington University developed a statistical model to measure systematic bias in the behavior of Wikipedia's users regarding controversial topics. The authors focused on behavioral changes of the encyclopedia's administrators after assuming the post, writing that systematic bias occurred after the fact. [207] [208]

Identifying the filter-bubble problem
Dimitra Kessenides, writing for Bloomberg News Weekly, identified the ' filter-bubble ' problem as a recurrent and long-standing issue at Wikipedia. [209] As Kessenides states: "If the only way to get an article about the developing world published on Wikipedia was to know a former board member, it was hard to imagine how a random editor in Johannesburg or Bangalore would have any hope... This so-called filter-bubble problem, coined by Eli Pariser , co-founder of the viral video site Upworthy , is the idea that the internet can contribute to the insularity of certain communities. Filter bubbles have been blamed for the spread of misinformation during the 2016 presidential election and for the failure of pundits in the U.K. to anticipate Brexit... Wikipedia's filter-bubble problem is a particularly acute threat for an organization whose stated mission is 'to empower and engage people around the world.'" [209]

Explicit content
Wikipedia has been criticized for allowing information of graphic content. Articles depicting arguably objectionable content (such as Feces , Cadaver , Human penis , Vulva , and Nudity ) contain graphic pictures and detailed information easily available to anyone with access to the internet, including children.
The site also includes sexual content such as images and videos of masturbation and ejaculation , photographs of nude children , illustrations of zoophilia , and photos from hardcore pornographic films in its articles.
The Wikipedia article about Virgin Killer — a 1976 album from German heavy metal band Scorpions —features a picture of the album's original cover, which depicts a naked prepubescent girl. The original release cover caused controversy and was replaced in some countries. In December 2008, access to the Wikipedia article Virgin Killer was blocked for four days by most Internet service providers in the United Kingdom after the Internet Watch Foundation (IWF) decided the album cover was a potentially illegal indecent image and added the article's url to a "blacklist" it supplies to British internet service providers. [211] That the IWF, a non-government-affiliated organization, had so much control was described as "alarming". [212]
In April 2010, Sanger wrote a letter to the Federal Bureau of Investigation, outlining his concerns that two categories of images on Wikimedia Commons contained child pornography, and were in violation of US federal obscenity law . [213] [214] Sanger later clarified that the images, which were related to pedophilia and one about lolicon , were not of real children, but said that they constituted "obscene visual representations of the sexual abuse of children", under the PROTECT Act of 2003 . [215] That law bans photographic child pornography and cartoon images and drawings of children that are obscene under American law . [215] Sanger also expressed concerns about access to the images on Wikipedia in schools. [216] Wikimedia Foundation spokesman Jay Walsh strongly rejected Sanger's accusation, [217] saying that Wikipedia did not have "material we would deem to be illegal. If we did, we would remove it." [217] Following the complaint by Sanger, Wales deleted sexual images without consulting the community. After some editors who volunteer to maintain the site argued that the decision to delete had been made hastily, Wales voluntarily gave up some of the powers he had held up to that time as part of his co-founder status. He wrote in a message to the Wikimedia Foundation mailing-list that this action was "in the interest of encouraging this discussion to be about real philosophical/content issues, rather than be about me and how quickly I acted". [218] Critics, including Wikipediocracy , noticed that many of the pornographic images deleted from Wikipedia since 2010 have reappeared. [219]

Privacy
One privacy concern in the case of Wikipedia is the right of a private citizen to remain a "private citizen" rather than a " public figure " in the eyes of the law. [220] [notes 9] It is a battle between the right to be anonymous in cyberspace and the right to be anonymous in real life (" meatspace "). A particular problem occurs in the case of an individual who is relatively unimportant and for whom there exists a Wikipedia page against her or his wishes.
In January 2006, a German court ordered the German Wikipedia shut down within Germany because it stated the full name of Boris Floricic , aka "Tron", a deceased hacker. On February 9, 2006, the injunction against Wikimedia Deutschland was overturned, with the court rejecting the notion that Tron's right to privacy or that of his parents was being violated. [221]
Wikipedia has a " Volunteer Response Team " that uses the OTRS system to handle queries without having to reveal the identities of the involved parties. This is used, for example, in confirming the permission for using individual images and other media in the project. [222]

Sexism
Wikipedia has been described as harboring a battleground culture of sexism and harassment . [223] [224] The perceived toxic attitudes and tolerance of violent and abusive language are also reasons put forth for the gender gap in Wikipedia editors. [225] In 2014, a female editor who requested a separate space on Wikipedia to discuss improving civility had her proposal referred to by a male editor using the words "the easiest way to avoid being called a cunt is not to act like one." [226]

Operation
A group of Wikipedia editors may form a WikiProject to focus their work on a specific topic area, using its associated discussion page to coordinate changes across multiple articles. [227]

Wikimedia Foundation and Wikimedia movement affiliates
Wikipedia is hosted and funded by the Wikimedia Foundation , a non-profit organization which also operates Wikipedia-related projects such as Wiktionary and Wikibooks . The foundation relies on public contributions and grants to fund its mission. [228] The foundation's 2013 IRS Form 990 shows revenue of $39.7 million and expenses of almost $29 million, with assets of $37.2 million and liabilities of about $2.3 million. [229]
In May 2014, Wikimedia Foundation named Lila Tretikov as its second executive director, taking over for Sue Gardner. [230] The Wall Street Journal reported on May 1, 2014, that Tretikov's information technology background from her years at University of California offers Wikipedia an opportunity to develop in more concentrated directions guided by her often repeated position statement that, "Information, like air, wants to be free." [231] [232] The same Wall Street Journal article reported these directions of development according to an interview with spokesman Jay Walsh of Wikimedia, who "said Tretikov would address that issue ( paid advocacy ) as a priority. 'We are really pushing toward more transparency... We are reinforcing that paid advocacy is not welcome.' Initiatives to involve greater diversity of contributors, better mobile support of Wikipedia, new geo-location tools to find local content more easily, and more tools for users in the second and third world are also priorities, Walsh said." [231]
Following the departure of Tretikov from Wikipedia due to issues concerning the use of the "superprotection" feature which some language versions of Wikipedia have adopted, Katherine Maher became the third executive director the Wikimedia Foundation in June 2016. [233] Maher has stated that one of her priorities would be the issue of editor harassment endemic to Wikipedia as identified by the Wikipedia board in December. Maher stated regarding the harassment issue that: "It establishes a sense within the community that this is a priority... (and that correction requires that) it has to be more than words." [234]
Wikipedia is also supported by many organizations and groups that are affiliated with the Wikimedia Foundation but independently-run, called Wikimedia movement affiliates . These include Wikimedia chapters (which are national or sub-national organizations, such as Wikimedia Deutschland and Wikimédia France), thematic organizations (such as Amical Wikimedia for the Catalan language community), and user groups. These affiliates participate in the promotion, development, and funding of Wikipedia.

Software operations and support
The operation of Wikipedia depends on MediaWiki , a custom-made, free and open source wiki software platform written in PHP and built upon the MySQL database system. [235] The software incorporates programming features such as a macro language , variables , a transclusion system for templates , and URL redirection . MediaWiki is licensed under the GNU General Public License and it is used by all Wikimedia projects, as well as many other wiki projects. Originally, Wikipedia ran on UseModWiki written in Perl by Clifford Adams (Phase I), which initially required CamelCase for article hyperlinks; the present double bracket style was incorporated later. Starting in January 2002 (Phase II), Wikipedia began running on a PHP wiki engine with a MySQL database; this software was custom-made for Wikipedia by Magnus Manske . The Phase II software was repeatedly modified to accommodate the exponentially increasing demand. In July 2002 (Phase III), Wikipedia shifted to the third-generation software, MediaWiki, originally written by Lee Daniel Crocker .
Several MediaWiki extensions are installed [236] to extend the functionality of the MediaWiki software.
In April 2005, a Lucene extension [237] [238] was added to MediaWiki's built-in search and Wikipedia switched from MySQL to Lucene for searching. The site currently uses Lucene Search 2.1, [239] [ needs update ] which is written in Java and based on Lucene library 2.3. [240]
In July 2013, after extensive beta testing, a WYSIWYG (What You See Is What You Get) extension, VisualEditor , was opened to public use. [241] [242] [243] [244] It was met with much rejection and criticism, and was described as "slow and buggy". [245] The feature was changed from opt-out to opt-in afterward.

Automated editing
Computer programs called bots have been used widely to perform simple and repetitive tasks, such as correcting common misspellings and stylistic issues, or to start articles such as geography entries in a standard format from statistical data. [246] [247] [248] One controversial contributor massively creating articles with his bot was reported to create up to ten thousand articles on the Swedish Wikipedia on certain days. [249] There are also some bots designed to automatically notify editors when they make common editing errors (such as unmatched quotes or unmatched parenthesis). [250] Edits misidentified by a bot as the work of a banned editor can be restored by other editors. An anti-vandal bot tries to detect and revert vandalism quickly and automatically. [247] Bots can also report edits from particular accounts or IP address ranges, as was done at the time of the MH17 jet downing incident in July 2014. [251] Bots on Wikipedia must be approved prior to activation. [252]
According to Andrew Lih , the current expansion of Wikipedia to millions of articles would be difficult to envision without the use of such bots. [253]

Wikiprojects, and assessments of articles' importance and quality
A " WikiProject " is a group of contributors who want to work together as a team to improve Wikipedia. These groups often focus on a specific topic area (for example, women's history ), a specific location or a specific kind of task (for example, checking newly created pages). The English Wikipedia currently has over 2,000 WikiProjects and activity varies. [254]
In 2007, in preparation for producing a print version, the English Wikipedia introduced an assessment scale of the quality of articles. [255] Articles are rated by WikiProjects. The range of quality classes begins with "Stub" (very short pages), followed by "Start", "C" and "B" (in increasing order of quality). Community peer review is needed for the article to enter one of the highest quality classes: either "A", " good article " or the highest, " featured article ". Of the about 4.4 million articles and lists assessed as of March 2015, a little more than 5,000 (0.12%) are featured articles, and fewer than 2,000 (0.04%) are featured lists. One featured article per day, as selected by editors, appears on the main page of Wikipedia. [256] [257]
The articles can also be rated as per "importance" as judged by a WikiProject. Currently, there are 5 importance categories: "low", "mid", "high", "top", and "???" for unclassified/uncertain level. For a particular article, different WikiProjects may assign different importance levels.
The Wikipedia Version 1.0 Editorial Team has developed a table (shown below) that displays data of all rated articles by quality and importance, on the English Wikipedia. If an article or list receives different ratings by two or more WikiProjects, then the highest rating is used in the table, pie-charts, and bar-chart. The software regularly auto-updates the data.
Researcher Giacomo Poderi found that articles tend to reach featured status via the intensive work of a few editors. [258] A 2010 study found unevenness in quality among featured articles and concluded that the community process is ineffective in assessing the quality of articles. [259]
[Note: The table above (prepared by the Wikipedia Version 1.0 Editorial Team ) is automatically updated daily by User:WP 1.0 bot , but the bar-chart and the two pie-charts are not auto-updated. In them, new data has to be entered by a Wikipedia editor (i.e. user).]

Hardware operations and support
Wikipedia receives between 25,000 and 60,000 page requests per second, depending on time of day. [261] As of 2008 [update] page requests are first passed to a front-end layer of Squid caching servers. [262] [ needs update ] Further statistics, based on a publicly available 3-month Wikipedia access trace, are available. [263] Requests that cannot be served from the Squid cache are sent to load-balancing servers running the Linux Virtual Server software, which in turn pass them to one of the Apache web servers for page rendering from the database. The web servers deliver pages as requested, performing page rendering for all the language editions of Wikipedia. To increase speed further, rendered pages are cached in a distributed memory cache until invalidated, allowing page rendering to be skipped entirely for most common page accesses.
Wikipedia currently runs on dedicated clusters of Linux servers (mainly Ubuntu ). [264] [265] As of December 2009 [update] , there were 300 in Florida and 44 in Amsterdam . [266] By January 22, 2013, Wikipedia had migrated its primary data center to an Equinix facility in Ashburn, Virginia . [267] [268]

Internal research and operational development
In accordance with growing amounts of incoming donations exceeding seven digits in 2013 as recently reported, [50] the Foundation has reached a threshold of assets which qualify its consideration under the principles of industrial organization economics to indicate the need for the re-investment of donations into the internal research and development of the Foundation. [269] Two of the recent projects of such internal research and development have been the creation of a Visual Editor and a largely under-utilized "Thank" tab which were developed for the purpose of ameliorating issues of editor attrition, which have met with limited success. [50] [245] The estimates for reinvestment by industrial organizations into internal research and development was studied by Adam Jaffe, who recorded that the range of 4% to 25% annually was to be recommended, with high end technology requiring the higher level of support for internal reinvestment. [270] At the 2013 level of contributions for Wikimedia presently documented as 45 million dollars, the computed budget level recommended by Jaffe and Caballero for reinvestment into internal research and development is between 1.8 million and 11.3 million dollars annually. [270] In 2016, the level of contributions were reported by Blomberg News as being at $77 million annually, updating the Jaffe estimates for the higher level of support to between 3.08 million and 19.2 million dollars annually. [270]

Internal news publications
Community-produced news publications include the English Wikipedia's The Signpost , founded in 2005 by Michael Snow, an attorney, Wikipedia administrator and former chair of the Wikimedia Foundation board of trustees. [271] It covers news and events from the site, as well as major events from other Wikimedia projects , such as Wikimedia Commons . Similar publications are the German-language Kurier , and the Portuguese-language Correio da Wikipédia . Other past and present community news publications on English Wikipedia include the "Wikiworld" web comic, the Wikipedia Weekly podcast, and newsletters of specific WikiProjects like The Bugle from WikiProject Military History and the monthly newsletter from The Guild of Copy Editors . There are also a number of publications from the Wikimedia Foundation and multilingual publications such as the Wikimedia Blog and This Month in Education .

Access to content

Content licensing
When the project was started in 2001, all text in Wikipedia was covered by the GNU Free Documentation License (GFDL), a copyleft license permitting the redistribution, creation of derivative works, and commercial use of content while authors retain copyright of their work. [272] The GFDL was created for software manuals that come with free software programs licensed under the GPL . This made it a poor choice for a general reference work: for example, the GFDL requires the reprints of materials from Wikipedia to come with a full copy of the GFDL text. In December 2002, the Creative Commons license was released: it was specifically designed for creative works in general, not just for software manuals. The license gained popularity among bloggers and others distributing creative works on the Web. The Wikipedia project sought the switch to the Creative Commons. [273] Because the two licenses, GFDL and Creative Commons, were incompatible, in November 2008, following the request of the project, the Free Software Foundation (FSF) released a new version of the GFDL designed specifically to allow Wikipedia to relicense its content to CC BY-SA by August 1, 2009. (A new version of the GFDL automatically covers Wikipedia contents.) In April 2009, Wikipedia and its sister projects held a community-wide referendum which decided the switch in June 2009. [274] [275] [276] [277]
The handling of media files (e.g. image files) varies across language editions. Some language editions, such as the English Wikipedia, include non-free image files under fair use doctrine, while the others have opted not to, in part because of the lack of fair use doctrines in their home countries (e.g. in Japanese copyright law ). Media files covered by free content licenses (e.g. Creative Commons ' CC BY-SA) are shared across language editions via Wikimedia Commons repository, a project operated by the Wikimedia Foundation. Wikipedia's accommodation of varying international copyright laws regarding images has led some to observe that its photographic coverage of topics lags behind the quality of the encyclopedic text. [278]
The Wikimedia Foundation is not a licensor of content, but merely a hosting service for the contributors (and licensors) of the Wikipedia. This position has been successfully defended in court. [279] [280]

Methods of access
Because Wikipedia content is distributed under an open license, anyone can reuse or re-distribute it at no charge. The content of Wikipedia has been published in many forms, both online and offline, outside of the Wikipedia website.
Obtaining the full contents of Wikipedia for reuse presents challenges, since direct cloning via a web crawler is discouraged. [291] Wikipedia publishes "dumps" of its contents, but these are text-only; as of 2007 [update] there was no dump available of Wikipedia's images. [292]
Several languages of Wikipedia also maintain a reference desk , where volunteers answer questions from the general public. According to a study by Pnina Shachaf in the Journal of Documentation , the quality of the Wikipedia reference desk is comparable to a standard library reference desk , with an accuracy of 55%. [293]

Mobile access
Wikipedia's original medium was for users to read and edit content using any standard web browser through a fixed Internet connection . Although Wikipedia content has been accessible through the mobile web since July 2013, The New York Times on February 9, 2014, quoted Erik Möller, deputy director of the Wikimedia Foundation, stating that the transition of internet traffic from desktops to mobile devices was significant and a cause for concern and worry. [16] The article in The New York Times reported the comparison statistics for mobile edits stating that, "Only 20 percent of the readership of the English-language Wikipedia comes via mobile devices, a figure substantially lower than the percentage of mobile traffic for other media sites, many of which approach 50 percent. And the shift to mobile editing has lagged even more." [16] The New York Times reports that Möller has assigned "a team of 10 software developers focused on mobile", out of a total of approximately 200 employees working at the Wikimedia Foundation. One principal concern cited by The New York Times for the "worry" is for Wikipedia to effectively address attrition issues with the number of editors which the online encyclopedia attracts to edit and maintain its content in a mobile access environment. [16]
Bloomberg Businessweek reported in July 2014 that Google's Android mobile apps have dominated the largest share of global smartphone shipments for 2013 with 78.6% of market share over their next closest competitor in iOS with 15.2% of the market. [294] At the time of the Tretikov appointment and her posted web interview with Sue Gardner in May 2014, Wikimedia representatives made a technical announcement concerning the number of mobile access systems in the market seeking access to Wikipedia. Directly after the posted web interview, the representatives stated that Wikimedia would be applying an all-inclusive approach to accommodate as many mobile access systems as possible in its efforts for expanding general mobile access, including BlackBerry and the Windows Phone system, making market share a secondary issue. [232] The latest version of the Android app for Wikipedia was released on July 23, 2014, to generally positive reviews, scoring over four of a possible five in a poll of approximately 200,000 users downloading from Google. [295] The latest version for iOS was released on April 3, 2013, to similar reviews. [296]
Access to Wikipedia from mobile phones was possible as early as 2004, through the Wireless Application Protocol (WAP), via the Wapedia service. In June 2007 Wikipedia launched en.mobile.wikipedia.org , an official website for wireless devices. In 2009 a newer mobile service was officially released, [297] located at en.m.wikipedia.org , which caters to more advanced mobile devices such as the iPhone , Android -based devices or WebOS -based devices. Several other methods of mobile access to Wikipedia have emerged. Many devices and applications optimize or enhance the display of Wikipedia content for mobile devices, while some also incorporate additional features such as use of Wikipedia metadata (See Wikipedia:Metadata ), such as geoinformation . [298] [299]
Wikipedia Zero is an initiative of the Wikimedia Foundation to expand the reach of the encyclopedia to the developing countries. [300]
Andrew Lih and Andrew Brown both maintain editing Wikipedia with smart phones is difficult and this discourages new potential contributors. Several years running the number of Wikipedia editors has been falling and Tom Simonite of MIT Technology Review claims the bureaucratic structure and rules are a factor in this. Simonite alleges some Wikipedians use the labyrinthine rules and guidelines to dominate others and those editors have a vested interest in keeping the status quo . [50] Lih alleges there is serious disagreement among existing contributors how to resolve this. Lih fears for Wikipedia's long term future while Brown fears problems with Wikipedia will remain and rival encyclopedias will not replace it. [301] [302]

Cultural impact

Readership
Wikipedia is extremely popular. In February 2014, The New York Times reported that Wikipedia is ranked fifth globally among all websites, stating "With 18 billion page views and nearly 500 million unique visitors a month […] Wikipedia trails just Yahoo, Facebook, Microsoft and Google, the largest with 1.2 billion unique visitors." [16]
In addition to logistic growth in the number of its articles, [303] Wikipedia has steadily gained status as a general reference website since its inception in 2001. [304] About 50% of search engine traffic to Wikipedia comes from Google, [305] a good portion of which is related to academic research. [306] The number of readers of Wikipedia worldwide reached 365 million at the end of 2009. [307] The Pew Internet and American Life project found that one third of US Internet users consulted Wikipedia. [308] In 2011 Business Insider gave Wikipedia a valuation of $4 billion if it ran advertisements. [309]
According to "Wikipedia Readership Survey 2011", the average age of Wikipedia readers is 36, with a rough parity between genders. Almost half of Wikipedia readers visit the site more than five times a month, and a similar number of readers specifically look for Wikipedia in search engine results. About 47% of Wikipedia readers do not realize that Wikipedia is a non-profit organization. [310]

Cultural significance
Wikipedia's content has also been used in academic studies, books, conferences, and court cases. [311] [312] [313] The Parliament of Canada 's website refers to Wikipedia's article on same-sex marriage in the "related links" section of its "further reading" list for the Civil Marriage Act . [314] The encyclopedia's assertions are increasingly used as a source by organizations such as the US federal courts and the World Intellectual Property Organization [315] – though mainly for supporting information rather than information decisive to a case. [316] Content appearing on Wikipedia has also been cited as a source and referenced in some US intelligence agency reports. [317] In December 2008, the scientific journal RNA Biology launched a new section for descriptions of families of RNA molecules and requires authors who contribute to the section to also submit a draft article on the RNA family for publication in Wikipedia. [318]
Wikipedia has also been used as a source in journalism, [319] [320] often without attribution, and several reporters have been dismissed for plagiarizing from Wikipedia. [321] [322] [323]
In 2006, Time magazine recognized Wikipedia's participation (along with YouTube , Reddit , MySpace , and Facebook [324] ) in the rapid growth of online collaboration and interaction by millions of people worldwide.
In July 2007 Wikipedia was the focus of a 30-minute documentary on BBC Radio 4 [325] which argued that, with increased usage and awareness, the number of references to Wikipedia in popular culture is such that the word is one of a select band of 21st-century nouns that are so familiar ( Google , Facebook, YouTube) that they no longer need explanation.
On September 28, 2007, Italian politician Franco Grillini raised a parliamentary question with the minister of cultural resources and activities about the necessity of freedom of panorama . He said that the lack of such freedom forced Wikipedia, "the seventh most consulted website", to forbid all images of modern Italian buildings and art, and claimed this was hugely damaging to tourist revenues. [326]
On September 16, 2007, The Washington Post reported that Wikipedia had become a focal point in the 2008 US election campaign , saying: "Type a candidate's name into Google, and among the first results is a Wikipedia page, making those entries arguably as important as any ad in defining a candidate. Already, the presidential entries are being edited, dissected and debated countless times each day." [327] An October 2007 Reuters article, titled "Wikipedia page the latest status symbol", reported the recent phenomenon of how having a Wikipedia article vindicates one's notability. [328]
Active participation also has an impact. Law students have been assigned to write Wikipedia articles as an exercise in clear and succinct writing for an uninitiated audience. [329]
A working group led by Peter Stone (formed as a part of the Stanford -based project One Hundred Year Study on Artificial Intelligence ) in its report called Wikipedia "the best-known example of crowdsourcing... that far exceeds traditionally-compiled information sources, such as encyclopedias and dictionaries, in scale and depth." [330]

Awards
Wikipedia won two major awards in May 2004. [331] The first was a Golden Nica for Digital Communities of the annual Prix Ars Electronica contest; this came with a €10,000 (£6,588; $12,700) grant and an invitation to present at the PAE Cyberarts Festival in Austria later that year. The second was a Judges' Webby Award for the "community" category. [332] Wikipedia was also nominated for a "Best Practices" Webby award.
In 2007, readers of brandchannel.com voted Wikipedia as the fourth-highest brand ranking, receiving 15% of the votes in answer to the question "Which brand had the most impact on our lives in 2006?" [333]
In September 2008, Wikipedia received Quadriga A Mission of Enlightenment award of Werkstatt Deutschland along with Boris Tadić , Eckart Höfling , and Peter Gabriel . The award was presented to Wales by David Weinberger . [334]
In 2015, Wikipedia was awarded both the annual Erasmus Prize , which recognizes exceptional contributions to culture, society or social sciences, [335] and the Spanish Princess of Asturias Award on International Cooperation. [336] Speaking at the Asturian Parliament in Oviedo, the city that hosts the awards ceremony, Jimmy Wales praised the work of the Asturian language Wikipedia users. [337] The night of the ceremony, members of the Wikimedia Foundation held a meeting with Wikipedians from all parts of Spain, including the local Asturian community .

Satire
Many parodies target Wikipedia's openness and susceptibility to inserted inaccuracies, with characters vandalizing or modifying the online encyclopedia project's articles.
Comedian Stephen Colbert has parodied or referenced Wikipedia on numerous episodes of his show The Colbert Report and coined the related term wikiality , meaning "together we can create a reality that we all agree on—the reality we just agreed on". [177] Another example can be found in "Wikipedia Celebrates 750 Years of American Independence", a July 2006 front-page article in The Onion , [338] as well as the 2010 The Onion article "'L.A. Law' Wikipedia Page Viewed 874 Times Today". [339]
In an episode of the television comedy The Office U.S. , which aired in April 2007, an incompetent office manager ( Michael Scott ) is shown relying on a hypothetical Wikipedia article for information on negotiation tactics in order to assist him in negotiating lesser pay for an employee. [340] The tactics he used failed, as a joke about the unreliability of Wikipedia and what anyone can do to change its contents. Viewers of the show tried to add the episode's mention of the page as a section of the actual Wikipedia article on negotiation, but this effort was prevented by other users on the article's talk page. [341]
" My Number One Doctor ", a 2007 episode of the television show Scrubs , played on the perception that Wikipedia is an unreliable reference tool with a scene in which Dr. Perry Cox reacts to a patient who says that a Wikipedia article indicates that the raw food diet reverses the effects of bone cancer by retorting that the same editor who wrote that article also wrote the Battlestar Galactica episode guide . [342]
In 2008, the comedic website CollegeHumor produced a video sketch named "Professor Wikipedia", in which the fictitious Professor Wikipedia instructs a class with a medley of unverifiable and occasionally absurd statements. [343]
The Dilbert comic strip from May 8, 2009, features a character supporting an improbable claim by saying "Give me ten minutes and then check Wikipedia." [344]
In July 2009, BBC Radio 4 broadcast a comedy series called Bigipedia , which was set on a website which was a parody of Wikipedia. Some of the sketches were directly inspired by Wikipedia and its articles. [345]
In 2010, comedian Daniel Tosh encouraged viewers of his show, Tosh.0 , to visit the show's Wikipedia article and edit it at will. On a later episode, he commented on the edits to the article, most of them offensive, which had been made by the audience and had prompted the article to be locked from editing. [346] [347]
On August 23, 2013, the New Yorker website published a cartoon with this caption: "Dammit, Manning, have you considered the pronoun war that this is going to start on your Wikipedia page?" [348]
In December 2015, John Julius Norwich stated, in a letter published in The Times newspaper, that as an historian he resorted to Wikipedia "at least a dozen times a day", and had never yet caught it out. He described it as "a work of reference as useful as any in existence", with so wide a range that it is almost impossible to find a person, place or thing that it has left uncovered, and that he could never have written his last two books without it. [349] [350]

Sister projects – Wikimedia
Wikipedia has also spawned several sister projects, which are also wikis run by the Wikimedia Foundation . These other Wikimedia projects include Wiktionary , a dictionary project launched in December 2002, [351] Wikiquote , a collection of quotations created a week after Wikimedia launched, Wikibooks , a collection of collaboratively written free textbooks and annotated texts, Wikimedia Commons , a site devoted to free-knowledge multimedia, Wikinews , for citizen journalism, and Wikiversity , a project for the creation of free learning materials and the provision of online learning activities. [352] Another sister project of Wikipedia, Wikispecies , is a catalogue of species. In 2012 Wikivoyage , an editable travel guide, and Wikidata , an editable knowledge base, launched.

Publishing
The most obvious economic effect of Wikipedia has been the death of commercial encyclopedias, especially the printed versions, e.g. Encyclopædia Britannica , which were unable to compete with a product that is essentially free. [353] [354] [355] Nicholas Carr wrote a 2005 essay, "The amorality of Web 2.0 ", that criticized websites with user-generated content , like Wikipedia, for possibly leading to professional (and, in his view, superior) content producers' going out of business, because "free trumps quality all the time". Carr wrote: "Implicit in the ecstatic visions of Web 2.0 is the hegemony of the amateur. I for one can't imagine anything more frightening." [356] Others dispute the notion that Wikipedia, or similar efforts, will entirely displace traditional publications. For instance, Chris Anderson , the editor-in-chief of Wired Magazine , wrote in Nature that the " wisdom of crowds " approach of Wikipedia will not displace top scientific journals , with their rigorous peer review process. [357]
There is also an ongoing debate about the influence of Wikipedia on the biography publishing business. "The worry is that, if you can get all that information from Wikipedia, what's left for biography?" said Kathryn Hughes , professor of life writing at UEA and author of The Short Life and Long Times of Mrs Beeton and George Eliot: the Last Victorian . [358]

Scientific use
Wikipedia has seen been widely used as a corpus for linguistic research in computational linguistics , information retrieval and natural language processing . In particular, it commonly serves as a target knowledge base for the entity linking problem, which is then called "wikification", [359] and to the related problem of word sense disambiguation . [360] Methods similar to wikification can in turn be used to find "missing" links in Wikipedia. [361]
In 2015, French researchers Dr José Lages of the University of Franche-Comté in Besançon and Dima Shepelyansky of Paul Sabatier University in Toulouse published a global university ranking based on Wikipedia scholarly citations. [362] [363] [364] They used PageRank "followed by the number of appearances in the 24 different language editions of Wikipedia (descending order) and the century in which they were founded (ascending order)." [364]

Related projects
A number of interactive multimedia encyclopedias incorporating entries written by the public existed long before Wikipedia was founded. The first of these was the 1986 BBC Domesday Project , which included text (entered on BBC Micro computers) and photographs from over 1 million contributors in the UK, and covered the geography, art, and culture of the UK. This was the first interactive multimedia encyclopedia (and was also the first major multimedia document connected through internal links), with the majority of articles being accessible through an interactive map of the UK. The user interface and part of the content of the Domesday Project were emulated on a website until 2008. [365]
Several free-content, collaborative encyclopedias were created around the same period as Wikipedia (e.g. Everything2 ), [366] with many later being merged into the project (e.g. GNE ). [367] One of the most successful early online encyclopedias incorporating entries by the public was h2g2 , which was created by Douglas Adams in 1999. The h2g2 encyclopedia is relatively light-hearted, focusing on articles which are both witty and informative.
Subsequent collaborative knowledge websites have drawn inspiration from Wikipedia. Some, such as Susning.nu , Enciclopedia Libre , Hudong , and Baidu Baike likewise employ no formal review process, although some like Conservapedia are not as open. Others use more traditional peer review , such as Encyclopedia of Life and the online wiki encyclopedias Scholarpedia and Citizendium . The latter was started by Sanger in an attempt to create a reliable alternative to Wikipedia. [368] [369]

See also
WebPage index: 00002
English language
English i / ˈ ɪ ŋ ɡ l ɪ ʃ / is a West Germanic language that was first spoken in early medieval England and is now the global lingua franca . [4] [5] Named after the Angles , one of the Germanic tribes that migrated to England , it ultimately derives its name from the Anglia (Angeln) peninsula in the Baltic Sea . It is most closely related to the Frisian languages , but its vocabulary has been significantly influenced by other Germanic languages in the early medieval period, and later by Romance languages , particularly Latin and French . [6]
English is either the official language or one of the official languages in almost 60 sovereign states . It is the most commonly spoken language in the United Kingdom, the United States, Canada, Australia, Ireland, and New Zealand, and is widely spoken in some areas of the Caribbean , Africa, and South Asia. [7] It is the third most common native language in the world, after Mandarin and Spanish . [8] It is the most widely learned second language and an official language of the United Nations , of the European Union , and of many other world and regional international organisations. It is the most widely spoken Germanic language, accounting for at least 70% of speakers of this Indo-European branch.
English has developed over the course of more than 1,400 years. The earliest forms of English, a set of Anglo-Frisian dialects brought to Great Britain by Anglo-Saxon settlers in the fifth century, are called Old English . Middle English began in the late 11th century with the Norman conquest of England , and was a period in which the language was influenced by French. [9] Early Modern English began in the late 15th century with the introduction of the printing press to London and the King James Bible , and the start of the Great Vowel Shift . [10] Through the worldwide influence of the British Empire , modern English spread around the world from the 17th to mid-20th centuries. Through all types of printed and electronic media, as well as the emergence of the United States as a global superpower , English has become the leading language of international discourse and the lingua franca in many regions and in professional contexts such as science, navigation, and law. [11]
Modern English grammar is the result of a gradual change from a typical Indo-European dependent marking pattern with a rich inflectional morphology and relatively free word order , to a mostly analytic pattern with little inflection , a fairly fixed SVO word order and a complex syntax . [12] Modern English relies more on auxiliary verbs and word order for the expression of complex tenses , aspect and mood , as well as passive constructions , interrogatives and some negation . Despite noticeable variation among the accents and dialects of English used in different countries and regions – in terms of phonetics and phonology , and sometimes also vocabulary , grammar and spelling – English-speakers from around the world are able to communicate with one another with relative ease .

Classification
English is an Indo-European language , and belongs to the West Germanic group of the Germanic languages . [13] Apart from Scots and the extinct Fingallian and Forth and Bargy (Yola) dialects of Ireland (the other Anglic languages), English is most closely related to the three Frisian languages : West Frisian , North Frisian and Saterland Frisian , with which English forms the Anglo-Frisian subgroup within West Germanic. [14] [15] Low German (Low Saxon) ( nr. 19-24 on the map ), which evolved from Old Saxon , and whose language area also extends over the core region once inhabited by the Angles and Saxons ( Schleswig-Holstein with Anglia , and Lower Saxony ), is also very closely related; and sometimes Low German, English, and Frisian are grouped together as the Ingvaeonic (North Sea Germanic) languages. [16]
Furthermore, English is closely related to the other West Germanic languages: German , Dutch and Afrikaans . While Dutch and Afrikaans are classified as Franconian languages ( nr. 25, 29 and 32 ), Standard German is based on Thuringian - Upper Saxon dialects ( nr. 30 ), and therefore the three languages are about equally closely related to the Ingvaeonic languages. Since the Central German dialects ( nr. 29-31 ) Standard German is based on, were, other than English, Dutch and Afrikaans (and also Low German), affected by the High German consonant shift , German seems more distantly related to English than Dutch and Afrikaans, since cognates often are not so easily recognizable, but still, English and German resemble each other to a high degree.
Example sentence in 1) English, 2) Low German (Low Saxon) and 3) German ( ß = an s-sound ):
After the West Germanic languages, English is also related to the North Germanic languages : Danish , Norwegian , Swedish , Faroese and Icelandic .
The main criterion that differentiates English from all other Germanic languages is the considerably higher amount of Romance derived vocabulary. This can be explained through a heavy borrowing of French loanwords that occurred in the time after the Norman conquest of England (the Normans being Scandinavians that adopted French as their first language). Apart from being the most Romance Germanic language in terms of vocabulary, English also is the most North Germanic influenced West Germanic language, and, as an insular language, it developed independently of the continental West Germanic Frisian, Low German, German and Dutch, and is thus, differing in vocabulary , syntax and phonology , not mutually intelligible with these languages.
Because English through its history has changed considerably in response to contact with Old Norse and Norman French , some scholars have argued that English can be considered a mixed language or a creole – a theory called the Middle English creole hypothesis . Although the high degree of influence from these languages on the vocabulary and grammar of Modern English is widely acknowledged, most specialists in language contact do not consider English to be a true mixed language. [17] [18]
English is classified as a Germanic language because it shares new language features (different from other Indo-European languages) with other Germanic languages. [19] These shared innovations show that the languages have descended from a single common ancestor, which linguists call Proto-Germanic . Some shared features of Germanic languages are the use of modal verbs , the division of verbs into strong and weak classes, and the sound changes affecting Proto-Indo-European consonants, known as Grimm's and Verner's laws . Through Grimm's law, the word for foot begins with /f/ in Germanic languages, but its cognates in other Indo-European languages begin with /p/ . English is classified as an Anglo-Frisian language because Frisian and English share other features, such as the palatalisation of consonants that were velar consonants in Proto-Germanic (see Phonological history of Old English § Palatalization ). [20]

History

Proto-Germanic to Old English
The earliest form of English is called Old English or Anglo-Saxon (c. 550–1066 CE). Old English developed from a set of North Sea Germanic dialects originally spoken along the coasts of Frisia , Lower Saxony , Jutland , and Southern Sweden by Germanic tribes known as the Angles , Saxons , and Jutes . In the fifth century, the Anglo-Saxons settled Britain and the Romans withdrew from Britain . By the seventh century, the Germanic language of the Anglo-Saxons became dominant in Britain, replacing the languages of Roman Britain (43–409 CE): Common Brittonic , a Celtic language , and Latin , brought to Britain by the Roman occupation . [21] [22] [23] England and English (originally Ænglaland and Ænglisc ) are named after the Angles. [24]
Old English was divided into four dialects: the Anglian dialects, Mercian and Northumbrian , and the Saxon dialects, Kentish and West Saxon . [25] Through the educational reforms of King Alfred in the ninth century and the influence of the kingdom of Wessex , the West Saxon dialect became the standard written variety . [26] The epic poem Beowulf is written in West Saxon, and the earliest English poem, Cædmon's Hymn , is written in Northumbrian. [27] Modern English developed mainly from Mercian, but the Scots language developed from Northumbrian. A few short inscriptions from the early period of Old English were written using a runic script . [28] By the sixth century, a Latin alphabet was adopted, written with half-uncial letterforms . It included the runic letters wynn ⟨ ƿ ⟩ and thorn ⟨ þ ⟩, and the modified Latin letters eth ⟨ ð ⟩, and ash ⟨ æ ⟩. [28] [29]
Old English is very different from Modern English and difficult for 21st-century English speakers to understand. Its grammar was similar to that of modern German , and its closest relative is Old Frisian . Nouns, adjectives, pronouns, and verbs had many more inflectional endings and forms , and word order was much freer than in Modern English. Modern English has case forms in pronouns ( he , him , his ) and a few verb endings ( I have , he has ), but Old English had case endings in nouns as well, and verbs had more person and number endings. [30] [31] [32]
The translation of Matthew 8:20 from 1000 CE shows examples of case endings ( nominative plural, accusative plural, genitive singular) and a verb ending ( present plural):

Middle English
In the period from the 8th to the 12th century, Old English gradually transformed through language contact into Middle English . Middle English is often arbitrarily defined as beginning with the conquest of England by William the Conqueror in 1066, but it developed further in the period from 1200–1450.
First, the waves of Norse colonisation of northern parts of the British Isles in the 8th and 9th centuries put Old English into intense contact with Old Norse , a North Germanic language. Norse influence was strongest in the Northeastern varieties of Old English spoken in the Danelaw area around York, which was the centre of Norse colonisation; today these features are still particularly present in Scots and Northern English . However the centre of norsified English seems to have been in the Midlands around Lindsey , and after 920 CE when Lindsey was reincorporated into the Anglo-Saxon polity, Norse features spread from there into English varieties that had not been in intense contact with Norse speakers. Some elements of Norse influence that persist in all English varieties today are the pronouns beginning with th- ( they, them, their ) which replaced the Anglo-Saxon pronouns with h- ( hie, him, hera ). [35]
With the Norman conquest of England in 1066, the now norsified Old English language was subject to contact with the Old Norman language, a Romance language closely related to Modern French . The Norman language in England eventually developed into Anglo-Norman . Because Norman was spoken primarily by the elites and nobles, while the lower classes continued speaking Anglo-Saxon, the influence of Norman consisted of introducing a wide range of loanwords related to politics, legislation and prestigious social domains. [36] Middle English also greatly simplified the inflectional system, probably in order to reconcile Old Norse and Old English, which were inflectionally different but morphologically similar. The distinction between nominative and accusative case was lost except in personal pronouns, the instrumental case was dropped, and the use of the genitive case was limited to describing possession . The inflectional system regularised many irregular inflectional forms, [37] and gradually simplified the system of agreement, making word order less flexible. [38] By the Wycliffe Bible of the 1380s, the passage Matthew 8:20 was written
Here the plural suffix -n on the verb have is still retained, but none of the case endings on the nouns are present.
By the 12th century Middle English was fully developed, integrating both Norse and Norman features; it continued to be spoken until the transition to early Modern English around 1500. Middle English literature includes Geoffrey Chaucer 's The Canterbury Tales , and Malory's Le Morte d'Arthur . In the Middle English period the use of regional dialects in writing proliferated, and dialect traits were even used for effect by authors such as Chaucer.

Early Modern English
The next period in the history of English was Early Modern English (1500–1700). Early Modern English was characterised by the Great Vowel Shift (1350–1700), inflectional simplification, and linguistic standardisation.
The Great Vowel Shift affected the stressed long vowels of Middle English. It was a chain shift , meaning that each shift triggered a subsequent shift in the vowel system. Mid and open vowels were raised , and close vowels were broken into diphthongs . For example, the word bite was originally pronounced as the word beet is today, and the second vowel in the word about was pronounced as the word boot is today. The Great Vowel Shift explains many irregularities in spelling, since English retains many spellings from Middle English, and it also explains why English vowel letters have very different pronunciations from the same letters in other languages. [40] [41]
English began to rise in prestige during the reign of Henry V . Around 1430, the Court of Chancery in Westminster began using English in its official documents , and a new standard form of Middle English, known as Chancery Standard , developed from the dialects of London and the East Midlands . In 1476, William Caxton introduced the printing press to England and began publishing the first printed books in London, expanding the influence of this form of English. [42] Literature from the Early Modern period includes the works of William Shakespeare and the translation of the Bible commissioned by King James I . Even after the vowel shift the language still sounded different from Modern English: for example, the consonant clusters /kn ɡn sw/ in knight , gnat , and sword were still pronounced. Many of the grammatical features that a modern reader of Shakespeare might find quaint or archaic represent the distinct characteristics of Early Modern English. [43]
In the 1611 King James Version of the Bible, written in Early Modern English, Matthew 8:20 says:
This exemplifies the loss of case and its effects on sentence structure (replacement with Subject-Verb-Object word order, and the use of of instead of the non-possessive genitive), and the introduction of loanwords from French ( ayre ) and word replacements ( bird originally meaning "nestling" had replaced OE fugol ).

Spread of Modern English
By the late 18th century, the British Empire had facilitated the spread of English through its colonies and geopolitical dominance. Commerce, science and technology, diplomacy, art, and formal education all contributed to English becoming the first truly global language. English also facilitated worldwide international communication. [44] [45] As England continued to form new colonies, these in turn became independent and developed their own norms for how to speak and write the language. English was adopted in North America, India, parts of Africa, Australasia, and many other regions. In the post-colonial period, some of the newly created nations that had multiple indigenous languages opted to continue using English as the official language to avoid the political difficulties inherent in promoting any one indigenous language above the others. [46] [47] [48] In the 20th century the growing economic and cultural influence of the United States and its status as a superpower following the Second World War has, along with worldwide broadcasting in English by the BBC [49] and other broadcasters, significantly accelerated the spread of the language across the planet. [50] [51] By the 21st century, English was more widely spoken and written than any language has ever been. [52]
A major feature in the early development of Modern English was the codification of explicit norms for standard usage, and their dissemination through official media such as public education and state sponsored publications. In 1755 Samuel Johnson published his A Dictionary of the English Language which introduced a standard set of spelling conventions and usage norms. In 1828, Noah Webster published the American Dictionary of the English language in an effort to establish a norm for speaking and writing American English that was independent from the British standard. Within Britain, non-standard or lower class dialect features were increasingly stigmatised, leading to the quick spread of the prestige varieties among the middle classes. [53]
In terms of grammatical evolution, Modern English has now reached a stage where the loss of case is almost complete (case is now only found in pronouns, such as he and him , she and her , who and whom ), and where SVO word-order is mostly fixed. [53] Some changes, such as the use of do-support have become universalised. (Earlier English did not use the word "do" as a general auxiliary as Modern English does; at first it was only used in question constructions where it was not obligatory. [54] Now, do-support with the verb have is becoming increasingly standardised.) The use of progressive forms in -ing , appears to be spreading to new constructions, and forms such as had been being built are becoming more common. Regularisation of irregular forms also slowly continues (e.g. dreamed instead of dreamt ), and analytical alternatives to inflectional forms are becoming more common (e.g. more polite instead of politer ). British English is also undergoing change under the influence of American English, fuelled by the strong presence of American English in the media and the prestige associated with the US as a world power. [55] [56] [57]

Geographical distribution
As of 2016, 400 million people spoke English as their first language , and 1.1 billion spoke it as a secondary language. [58] English is probably the third largest language by number of native speakers, after Mandarin and Spanish . [8] However, when combining native and non-native speakers it may, depending on the estimate used, be the most commonly spoken language in the world. [52] [59] [60] [61] English is spoken by communities on every continent and on oceanic islands in all the major oceans. [62] The countries in which English is spoken can be grouped into different categories by how English is used in each country. The "inner circle" [63] countries with many native speakers of English share an international standard of written English and jointly influence speech norms of English around the world. English does not belong to just one country, and it does not belong solely to descendants of English settlers. English is an official language of countries populated by few descendants of native speakers of English. It has also become by far the most important language of international communication when people who share no native language meet anywhere in the world.

Three circles of English-speaking countries
Braj Kachru distinguishes countries where English is spoken with a three circles model . [63] In his model, the "inner circle" countries are countries with large communities of native speakers of English, "outer circle" countries have small communities of native speakers of English but widespread use of English as a second language in education or broadcasting or for local official purposes, and "expanding circle" countries are countries where many learners learn English as a foreign language. Kachru bases his model on the history of how English spread in different countries, how users acquire English, and the range of uses English has in each country. The three circles change membership over time. [64]
Countries with large communities of native speakers of English (the inner circle) include Britain, the United States, Australia, Canada, Ireland, and New Zealand, where the majority speaks English, and South Africa, where a significant minority speaks English. The countries with the most native English speakers are, in descending order, the United States (at least 231 million), [65] the United Kingdom (60 million), [66] [67] [68] Canada (19 million), [69] Australia (at least 17 million), [70] South Africa (4.8 million), [71] Ireland (4.2 million), and New Zealand (3.7 million). [72] In these countries, children of native speakers learn English from their parents, and local people who speak other languages or new immigrants learn English to communicate in their neighbourhoods and workplaces. [73] The inner-circle countries provide the base from which English spreads to other countries in the world. [64]
Estimates of the number of English speakers who are second language and foreign-language speakers vary greatly from 470 million to more than 1,000 million depending on how proficiency is defined. [7] Linguist David Crystal estimates that non-native speakers now outnumber native speakers by a ratio of 3 to 1. [59] In Kachru's three-circles model, the "outer circle" countries are countries such as the Philippines , [74] Jamaica , [75] India, Pakistan [ citation needed ] , Singapore, [76] and Nigeria [77] [78] with a much smaller proportion of native speakers of English but much use of English as a second language for education, government, or domestic business, and where English is routinely used for school instruction and official interactions with the government. [79] Those countries have millions of native speakers of dialect continua ranging from an English-based creole to a more standard version of English. They have many more speakers of English who acquire English in the process of growing up through day by day use and listening to broadcasting, especially if they attend schools where English is the medium of instruction. Varieties of English learned by speakers who are not native speakers born to English-speaking parents may be influenced, especially in their grammar, by the other languages spoken by those learners. [73] Most of those varieties of English include words little used by native speakers of English in the inner-circle countries, [73] and they may have grammatical and phonological differences from inner-circle varieties as well. The standard English of the inner-circle countries is often taken as a norm for use of English in the outer-circle countries. [73]
In the three-circles model, countries such as Poland, China, Brazil, Germany, Japan, Indonesia, Egypt, and other countries where English is taught as a foreign language make up the "expanding circle". [80] The distinctions between English as a first language, as a second language, and as a foreign language are often debatable and may change in particular countries over time. [79] For example, in the Netherlands and some other countries of Europe, knowledge of English as a second language is nearly universal, with over 80 percent of the population able to use it, [81] and thus English is routinely used to communicate with foreigners and often in higher education. In these countries, although English is not used for government business, the widespread use of English in these countries puts them at the boundary between the "outer circle" and "expanding circle". English is unusual among world languages in how many of its users are not native speakers but speakers of English as a second or foreign language. [82] Many users of English in the expanding circle use it to communicate with other people from the expanding circle, so that interaction with native speakers of English plays no part in their decision to use English. [83] Non-native varieties of English are widely used for international communication, and speakers of one such variety often encounter features of other varieties. [84] Very often today a conversation in English anywhere in the world may include no native speakers of English at all, even while including speakers from several different countries. [85]

Pluricentric English
English is a pluricentric language , which means that no one national authority sets the standard for use of the language. [86] [87] [88] [89] But English is not a divided language, [90] despite a long-standing joke originally attributed to George Bernard Shaw that the United Kingdom and the United States are "two countries separated by a common language". [91] Spoken English, for example English used in broadcasting, generally follows national pronunciation standards that are also established by custom rather than by regulation. International broadcasters are usually identifiable as coming from one country rather than another through their accents , [92] but newsreader scripts are also composed largely in international standard written English . The norms of standard written English are maintained purely by the consensus of educated English-speakers around the world, without any oversight by any government or international organisation. [93] American listeners generally readily understand most British broadcasting, and British listeners readily understand most American broadcasting. Most English speakers around the world can understand radio programmes, television programmes, and films from many parts of the English-speaking world. [94] Both standard and nonstandard varieties of English can include both formal or informal styles, distinguished by word choice and syntax and use both technical and non-technical registers. [95]
The settlement history of the English-speaking inner circle countries outside Britain helped level dialect distinctions and produce koineised forms of English in South Africa, Australia, and New Zealand. [96] The majority of immigrants to the United States without British ancestry rapidly adopted English after arrival. Now the majority of the United States population are monolingual English speakers, [97] [65] although English has been given official status by only 30 of the 50 state governments of the US. [98] [99]

English as a global language
English has ceased to be an "English language" in the sense of belonging only to people who are ethnically English. [100] [101] Use of English is growing country-by-country internally and for international communication. Most people learn English for practical rather than ideological reasons. [102] Many speakers of English in Africa have become part of an "Afro-Saxon" language community that unites Africans from different countries. [103]
As decolonisation proceeded throughout the British Empire in the 1950s and 1960s, former colonies often did not reject English but rather continued to use it as independent countries setting their own language policies. [47] [48] [104] For example, the view of the English language among many Indians has gone from associating it with colonialism to associating it with economic progress, and English continues to be an official language of India. [105] English is also widely used in media and literature, and the number of English language books published annually in India is the third largest in the world after the US and UK. [106] However English is rarely spoken as a first language, numbering only around a couple hundred-thousand people, and less than 5% of the population speak fluent English in India. [107] [108] David Crystal claimed in 2004 that, combining native and non-native speakers, India now has more people who speak or understand English than any other country in the world, [109] but the number of English speakers in India is very uncertain, with most scholars concluding that the United States still has more speakers of English than India. [110]
Modern English, sometimes described as the first global lingua franca , [50] [111] is also regarded as the first world language . [112] [113] English is the world's most widely used language in newspaper publishing, book publishing, international telecommunications, scientific publishing, international trade, mass entertainment, and diplomacy. [113] English is, by international treaty, the basis for the required controlled natural languages [114] Seaspeak and Airspeak, used as international languages of seafaring [115] and aviation. [116] English used to have parity with French & German in scientific research, but now it dominates that field. [117] It achieved parity with French as a language of diplomacy at the Treaty of Versailles negotiations in 1919. [118] By the time of the foundation of the United Nations at the end of World War II , English had become pre-eminent [119] and is now the main worldwide language of diplomacy and international relations. [120] It is one of six official languages of the United Nations. [121] Many other worldwide international organisations, including the International Olympic Committee , specify English as a working language or official language of the organisation.
Many regional international organisations such as the European Free Trade Association , Association of Southeast Asian Nations (ASEAN), [51] and Asia-Pacific Economic Cooperation (APEC) set English as their organisation's sole working language even though most members are not countries with a majority of native English speakers. While the European Union (EU) allows member states to designate any of the national languages as an official language of the Union, in practice English is the main working language of EU organisations. [122]
Although in most countries English is not an official language, it is currently the language most often taught as a foreign language . [50] [51] In the countries of the EU, English is the most widely spoken foreign language in nineteen of the twenty-five member states where it is not an official language (that is, the countries other than the UK, Ireland and Malta ). In a 2012 official Eurobarometer poll, 38 percent of the EU respondents outside the countries where English is an official language said they could speak English well enough to have a conversation in that language. The next most commonly mentioned foreign language, French (which is the most widely known foreign language in the UK and Ireland), could be used in conversation by 12 percent of respondents. [123]
A working knowledge of English has become a requirement in a number of occupations and professions such as medicine [124] and computing. English has become so important in scientific publishing that more than 80 percent of all scientific journal articles indexed by Chemical Abstracts in 1998 were written in English, as were 90 percent of all articles in natural science publications by 1996 and 82 percent of articles in humanities publications by 1995. [125]
Specialised subsets of English arise spontaneously in international communities, for example, among international business people, as an auxiliary language . This has led some scholars to develop the study of English as an auxiliary languages. Globish uses a relatively small subset of English vocabulary (about 1500 words with highest use in international business English) in combination with the standard English grammar. Other examples include Simple English .
The increased use of the English language globally has had an effect on other languages, leading to some English words being assimilated into the vocabularies of other languages. This influence of English has led to concerns about language death , [126] and to claims of linguistic imperialism , [127] and has provoked resistance to the spread of English; however the number of speakers continues to increase because many people around the world think that English provides them with opportunities for better employment and improved lives. [128]
Although some scholars mention a possibility of future divergence of English dialects into mutually unintelligible languages, most think a more likely outcome is that English will continue to function as a koineised language in which the standard form unifies speakers from around the world. [129] English is used as the language for wider communication in countries around the world. [130] Thus English has grown in worldwide use much more than any constructed language proposed as an international auxiliary language , including Esperanto . [131] [132]

Phonology
The phonetics and phonology of English differ between dialects, usually without interfering with mutual communication. Phonological variation affects the inventory of phonemes (speech sounds that distinguish meaning), and phonetic variation is differences in pronunciation of the phonemes. [133] This overview mainly describes the standard pronunciations of the United Kingdom and the United States : Received Pronunciation (RP) and General American (GA) (See Section below on "Dialects, accents and varieties" ). The phonetic symbols used below are from the International Phonetic Alphabet (IPA). [134] [135] [136]

Consonants
Most English dialects share the same 24 consonant phonemes. The consonant inventory shown below is valid for Californian American English, [137] and for RP. [138]
* Conventionally transcribed /r/ .
In the table, when obstruents (stops, affricates, and fricatives) appear in pairs, such as /p b/ , /tʃ dʒ/ , and /s z/ , the first is fortis (strong) and the second is lenis (weak). Fortis obstruents, such as /p tʃ s/ are pronounced with more muscular tension and breath force than lenis consonants, such as /b dʒ z/ , and are always voiceless . Lenis consonants are partly voiced at the beginning and end of utterances, and fully voiced between vowels. Fortis stops such as /p/ have additional articulatory or acoustic features in most dialects: they are aspirated [pʰ] when they occur alone at the beginning of a stressed syllable, often unaspirated in other cases, and often unreleased [p̚ ] or pre-glottalised [ˀp] at the end of a syllable. In a single-syllable word, a vowel before a fortis stop is shortened: thus nip has a noticeably shorter vowel (phonetically, but not phonemically) than nib [nɪˑp̬] ( see below ). [139]
In RP, the lateral approximant /l/ , has two main allophones (pronunciation variants): the clear or plain [l] , as in light , and the dark or velarised [ɫ] , as in full . [140] GA has dark l in most cases. [141]
All sonorants (liquids /l, r/ and nasals /m, n, ŋ/ ) devoice when following a voiceless obstruent, and they are syllabic when following a consonant at the end of a word. [142]

Vowels
The pronunciation of vowels varies a great deal between dialects and is one of the most detectable aspects of a speaker's accent. The table below lists the vowel phonemes in Received Pronunciation (RP) and General American (GA), with examples of words in which they occur from lexical sets compiled by linguists. The vowels are represented with symbols from the International Phonetic Alphabet; those given for RP are standard in British dictionaries and other publications.
In RP, vowel length is phonemic; long vowels are marked with a triangular colon ⟨ ː ⟩ in the table above, such as the vowel of need [niːd] as opposed to bid [bɪd] . GA does not have long vowels.
In both RP and GA, vowels are phonetically shortened before fortis consonants in the same syllable , like /t tʃ f/ , but not before lenis consonants like /d dʒ v/ or in open syllables: thus, the vowels of rich [rɪ̆tʃ] , neat [niˑt] , and safe [sĕɪ̆f] are noticeably shorter than the vowels of ridge [rɪdʒ] , need [niːd] , and save [seɪv] , and the vowel of light [lăɪ̆t] is shorter than that of lie [laɪ] . Because lenis consonants are frequently voiceless at the end of a syllable, vowel length is an important cue as to whether the following consonant is lenis or fortis. [143]
The vowels /ɨ ə/ only occur in unstressed syllables and are a result of vowel reduction . Some dialects do not distinguish them, so that roses and comma end in the same vowel, a dialect feature called weak-vowel merger . GA has an unstressed r -coloured schwa /ɚ/ , as in butter [ˈbʌtɚ] , which in RP has the same vowel as the word-final vowel in comma .

Phonotactics
An English syllable includes a syllable nucleus consisting of a vowel sound. Syllable onset and coda (start and end) are optional. A syllable can start with up to three consonant sounds, as in sprint /sprɪnt/ , and end with up to four, as in texts /teksts/ . This gives an English syllable the following structure, (CCC)V(CCCC) where C represents a consonant and V a vowel. The consonants that may appear together in onsets or codas are restricted, as is the order in which they may appear. Onsets can only have four types of consonant clusters: a stop and approximant, as in play ; a voiceless fricative and approximant, as in fly or sly ; s and a voiceless stop, as in stay ; and s , a voiceless stop, and an approximant, as in string . [144] Clusters of nasal and stop are only allowed in codas. Clusters of obstruents always agree in voicing, and clusters of sibilants and of plosives with the same point of articulation are prohibited. Furthermore, several consonants have limited distributions: /h/ can only occur in syllable initial position, and /ŋ/ only in syllable final position. [145]

Stress, rhythm and intonation
Stress plays an important role in English. Certain syllables are stressed, while others are unstressed. Stress is a combination of duration, intensity, vowel quality, and sometimes changes in pitch. Stressed syllables are pronounced longer and louder than unstressed syllables, and vowels in unstressed syllables are frequently reduced while vowels in stressed syllables are not. [146] Some words, primarily short function words but also some modal verbs such as can , have weak and strong forms depending on whether they occur in stressed or non-stressed position within a sentence.
Stress in English is phonemic , and some pairs of words are distinguished by stress. For instance, the word contract is stressed on the first syllable ( / ˈ k ɒ n t r æ k t / KON -trakt ) when used as a noun, but on the last syllable ( / k ə n ˈ t r æ k t / kən- TRAKT ) for most meanings (for example, "reduce in size") when used as a verb. [147] [148] [149] Here stress is connected to vowel reduction : in the noun "contract" the first syllable is stressed and has the unreduced vowel /ɒ/ , but in the verb "contract" the first syllable is unstressed and its vowel is reduced to /ə/ . Stress is also used to distinguish between words and phrases, so that a compound word receives a single stress unit, but the corresponding phrase has two: e.g. to búrn óut versus a búrnout , and a hótdog versus a hót dóg . [150]
In terms of rhythm , English is generally described as a stress-timed language, meaning that the amount of time between stressed syllables tends to be equal. Stressed syllables are pronounced longer, but unstressed syllables (syllables between stresses) are shortened. Vowels in unstressed syllables are shortened as well, and vowel shortening causes changes in vowel quality : vowel reduction .

Regional variation
Varieties of English vary the most in pronunciation of vowels. The best known national varieties used as standards for education in non English-speaking countries are British (BrE) and American (AmE). Countries such as Canada , Australia , Ireland , New Zealand and South Africa have their own standard varieties which are less often used as standards for education internationally. Some differences between the various dialects are shown in the table "Varieties of Standard English and their features". [151]
English has undergone many historical sound changes , some of them affecting all varieties, and others affecting only a few. Most standard varieties are affected by the Great Vowel Shift , which changed the pronunciation of long vowels, but a few dialects have slightly different results. In North America, a number of chain shifts such as the Northern Cities Vowel Shift and Canadian Shift have produced very different vowel landscapes in some regional accents.
Some dialects have fewer or more consonant phonemes and phones than the standard varieties. Some conservative varieties like Scottish English have a voiceless [ ʍ ] sound in whine that contrasts with the voiced [w] in wine , but most other dialects pronounce both words with voiced [w] , a dialect feature called wine – whine merger . The unvoiced velar fricative sound /x/ is found in Scottish English, which distinguishes loch /lɔx/ from lock /lɔk/ . Accents like Cockney with " h -dropping" lack the glottal fricative /h/ , and dialects with th -stopping and th -fronting like African American Vernacular and Estuary English do not have the dental fricatives /θ, ð/ , but replace them with dental or alveolar stops /t, d/ or labiodental fricatives /f, v/ . [152] [153] Other changes affecting the phonology of local varieties are processes such as yod -dropping , yod -coalescence , and reduction of consonant clusters.
General American and Received Pronunciation vary in their pronunciation of historical /r/ after a vowel at the end of a syllable (in the syllable coda ). GA is a rhotic dialect , meaning that it pronounces /r/ at the end of a syllable, but RP is non-rhotic, meaning that it loses /r/ in that position. English dialects are classified as rhotic or non-rhotic depending on whether they elide /r/ like RP or keep it like GA. [154]
There is complex dialectal variation in words with the open front and open back vowels /æ ɑː ɒ ɔː/ . These four vowels are only distinguished in RP, Australia, New Zealand and South Africa. In GA, these vowels merge to three /æ ɑ ɔ/ , [155] and in Canadian English they merge to two /æ ɑ/ . [156] In addition, the words that have each vowel vary by dialect. The table "Dialects and open vowels" shows this variation with lexical sets in which these sounds occur.

Grammar
As is typical of an Indo-European language, English follows accusative morphosyntactic alignment . English distinguishes at least seven major word classes: verbs, nouns, adjectives, adverbs, determiners (i.e. articles), prepositions, and conjunctions. Some analyses add pronouns as a class separate from nouns, and subdivide conjunctions into subordinators and coordinators, and add the class of interjections. [157] English also has a rich set of auxiliary verbs, such as have and do , expressing the categories of mood and aspect. Questions are marked by do-support , wh-movement (fronting of question words beginning with wh -) and word order inversion with some verbs.
Some traits typical of Germanic languages persist in English, such as the distinction between irregularly inflected strong stems inflected through ablaut (i.e. changing the vowel of the stem, as in the pairs speak/spoke and foot/feet ) and weak stems inflected through affixation (such as love/loved , hand/hands ). Vestiges of the case and gender system are found in the pronoun system ( he/him, who/whom ) and in the inflection of the copula verb to be .
The seven word classes are exemplified in this sample sentence: [158]

Nouns and noun phrases
English nouns are only inflected for number and possession. New nouns can be formed through derivation or compounding. They are semantically divided into proper nouns (names) and common nouns. Common nouns are in turn divided into concrete and abstract nouns, and grammatically into count nouns and mass nouns . [159]
Most count nouns are inflected for plural number through the use of the plural suffix - s , but a few nouns have irregular plural forms. Mass nouns can only be pluralised through the use of a count noun classifier, e.g. one loaf of bread , two loaves of bread . [160]
Regular plural formation:
Irregular plural formation:
Possession can be expressed either by the possessive enclitic - s (also traditionally called a genitive suffix), or by the preposition of . Historically the -s possessive has been used for animate nouns, whereas the of possessive has been reserved for inanimate nouns. Today this distinction is less clear, and many speakers use - s also with inanimates. Orthographically the possessive -s is separated from the noun root with an apostrophe.
Possessive constructions:
Nouns can form noun phrases (NPs) where they are the syntactic head of the words that depend on them such as determiners, quantifiers, conjunctions or adjectives. [161] Noun phrases can be short, such as the man , composed only of a determiner and a noun. They can also include modifiers such as adjectives (e.g. red , tall , all ) and specifiers such as determiners (e.g. the , that ). But they can also tie together several nouns into a single long NP, using conjunctions such as and , or prepositions such as with , e.g. the tall man with the long red trousers and his skinny wife with the spectacles (this NP uses conjunctions, prepositions, specifiers and modifiers). Regardless of length, an NP functions as a syntactic unit. For example, the possessive enclitic can, in cases which do not lead to ambiguity, follow the entire noun phrase, as in The President of India's wife , where the enclitic follows India and not President .
The class of determiners is used to specify the noun they precede in terms of definiteness , where the marks a definite noun and a or an an indefinite one. A definite noun is assumed by the speaker to be already known by the interlocutor, whereas an indefinite noun is not specified as being previously known. Quantifiers, which include one , many , some and all , are used to specify the noun in terms of quantity or number. The noun must agree with the number of the determiner, e.g. one man (sg.) but all men (pl.). Determiners are the first constituents in a noun phrase. [162]

Adjectives
Adjectives modify a noun by providing additional information about their referents. In English, adjectives come before the nouns they modify and after determiners. [163] In Modern English, adjectives are not inflected, and they do not agree in form with the noun they modify, as adjectives in most other Indo-European languages do. For example, in the phrases the slender boy , and many slender girls , the adjective slender does not change form to agree with either the number or gender of the noun.
Some adjectives are inflected for degree of comparison , with the positive degree unmarked, the suffix -er marking the comparative, and -est marking the superlative: a small boy , the boy is smaller than the girl , that boy is the smallest . Some adjectives have irregular comparative and superlative forms, such as good , better , and best . Other adjectives have comparatives formed by periphrastic constructions , with the adverb more marking the comparative, and most marking the superlative: happier or more happy , the happiest or most happy . [164] There is some variation among speakers regarding which adjectives use inflected or periphrastic comparison, and some studies have shown a tendency for the periphrastic forms to become more common at the expense of the inflected form. [165]

Pronouns, case and person
English pronouns conserve many traits of case and gender inflection. The personal pronouns retain a difference between subjective and objective case in most persons ( I/me, he/him, she/her, we/us, they/them ) as well as a gender and animateness distinction in the third person singular (distinguishing he/she/it ). The subjective case corresponds to the Old English nominative case , and the objective case is used both in the sense of the previous accusative case (in the role of patient, or direct object of a transitive verb), and in the sense of the Old English dative case (in the role of a recipient or indirect object of a transitive verb). [166] [167] Subjective case is used when the pronoun is the subject of a finite clause, and otherwise the objective case is used. [168] While grammarians such as Henry Sweet [169] and Otto Jespersen [170] noted that the English cases did not correspond to the traditional Latin based system, some contemporary grammars, for example Huddleston & Pullum (2002) , retain traditional labels for the cases, calling them nominative and accusative cases respectively.
Possessive pronouns exist in dependent and independent forms; the dependent form functions as a determiner specifying a noun (as in my chair ), while the independent form can stand alone as if it were a noun (e.g. the chair is mine ). [171] The English system of grammatical person no longer has a distinction between formal and informal pronouns of address, and the forms for 2nd person plural and singular are identical except in the reflexive form. Some dialects have introduced innovative 2nd person plural pronouns such as y'all found in Southern American English and African American (Vernacular) English or youse and ye found in Irish English.
Pronouns are used to refer to entities deictically or anaphorically . A deictic pronoun points to some person or object by identifying it relative to the speech situation — for example the pronoun I identifies the speaker, and the pronoun you , the addressee. Anaphorical pronouns such as that refer back to an entity already mentioned or assumed by the speaker to be known by the audience, for example in the sentence I already told you that . The reflexive pronouns are used when the oblique argument is identical to the subject of a phrase (e.g. "he sent it to himself" or "she braced herself for impact"). [172]

Prepositions
Prepositional phrases (PP) are phrases composed of a preposition and one or more nouns, e.g. with the dog , for my friend , to school , in England . Prepositions have a wide range of uses in English. They are used to describe movement, place, and other relations between different entities, but they also have many syntactic uses such as introducing complement clauses and oblique arguments of verbs. For example, in the phrase I gave it to him , the preposition to marks the recipient, or Indirect Object of the verb to give . Traditionally words were only considered prepositions if they governed the case of the noun they preceded, for example causing the pronouns to use the objective rather than subjective form, "with her", "to me", "for us". But some contemporary grammars such as that of Huddleston & Pullum (2002 :598–600) no longer consider government of case to be the defining feature of the class of prepositions, rather defining prepositions as words that can function as the heads of prepositional phrases.

Verbs and verb phrases
English verbs are inflected for tense and aspect, and marked for agreement with third person singular subject. Only the copula verb to be is still inflected for agreement with the plural and first and second person subjects. [164] Auxiliary verbs such as have and be are paired with verbs in the infinitive, past, or progressive forms. They form complex tenses, aspects, and moods. Auxiliary verbs differ from other verbs in that they can be followed by the negation, and in that they can occur as the first constituent in a question sentence. [173] [174]
Most verbs have six inflectional forms. The primary forms are a plain present, a third person singular present, and a preterite (past) form. The secondary forms are a plain form used for the infinitive, a gerund–participle and a past participle. [175] The copula verb to be is the only verb to retain some of its original conjugation, and takes different inflectional forms depending on the subject. The first person present tense form is am , the third person singular form is and the form are is used second person singular and all three plurals. The only verb past participle is been and its gerund-participle is being .

Tense, aspect and mood
English has two primary tenses, past (preterit) and non-past. The preterit is inflected by using the preterit form of the verb, which for the regular verbs includes the suffix -ed , and for the strong verbs either the suffix -t or a change in the stem vowel. The non-past form is unmarked except in the third person singular, which takes the suffix -s . [173]
English does not have a morphologised future tense. [176] Futurity of action is expressed periphrastically with one of the auxiliary verbs will or shall . [177] Many varieties also use a near future constructed with the phrasal verb be going to . [178]
Further aspectual distinctions are encoded by the use of auxiliary verbs, primarily have and be , which encode the contrast between a perfect and non-perfect past tense ( I have run vs. I was running ), and compound tenses such as preterite perfect ( I had been running ) and present perfect ( I have been running ). [179]
For the expression of mood, English uses a number of modal auxiliaries, such as can , may , will , shall and the past tense forms could , might , would , should . There is also a subjunctive and an imperative mood, both based on the plain form of the verb (i.e. without the third person singular -s ), and which is used in subordinate clauses (e.g. subjunctive: It is important that he run every day ; imperative Run! ). [177]
An infinitive form, that uses the plain form of the verb and the preposition to , is used for verbal clauses that are syntactically subordinate to a finite verbal clause. Finite verbal clauses are those that are formed around a verb in the present or preterit form. In clauses with auxiliary verbs they are the finite verbs and the main verb is treated as a subordinate clause. For example, he has to go where only the auxiliary verb have is inflected for time and the main verb to go is in the infinitive, or in a complement clause such as I saw him leave , where the main verb is to see which is in a preterite form, and leave is in the infinitive.

Phrasal verbs
English also makes frequent use of constructions traditionally called phrasal verbs , verb phrases that are made up of a verb root and a preposition or particle which follows the verb. The phrase then functions as a single predicate. In terms of intonation the preposition is fused to the verb, but in writing it is written as a separate word. Examples of phrasal verbs are to get up , to ask out , to back up , to give up , to get together , to hang out , to put up with , etc. The phrasal verb frequently has a highly idiomatic meaning that is more specialised and restricted than what can be simply extrapolated from the combination of verb and preposition complement (e.g. lay off meaning terminate someone's employment ). [180] In spite of the idiomatic meaning, some grammarians, including Huddleston & Pullum (2002) :274, do not consider this type of construction to form a syntactic constituent and hence refrain from using the term "phrasal verb". Instead they consider the construction simply to be a verb with a prepositional phrase as its syntactic complement, i.e. he woke up in the morning and he ran up in the mountains are syntactically equivalent.

Adverbs
The function of adverbs is to modify the action or event described by the verb by providing additional information about the manner in which it occurs. Many adverbs are derived from adjectives with the suffix -ly , but not all, and many speakers tend to omit the suffix in the most commonly used adverbs. For example, in the phrase the woman walked quickly the adverb quickly derived from the adjective quick describes the woman's way of walking. Some commonly used adjectives have irregular adverbial forms, such as good which has the adverbial form well .

Syntax
Modern English syntax language is moderately analytic . [181] It has developed features such as modal verbs and word order as resources for conveying meaning. Auxiliary verbs mark constructions such as questions, negative polarity, the passive voice and progressive aspect .

Basic constituent order
English word order has moved from the Germanic verb-second (V2) word order to being almost exclusively subject–verb–object (SVO). [182] The combination of SVO order and use of auxiliary verbs often creates clusters of two or more verbs at the centre of the sentence, such as he had hoped to try to open it .
In most sentences English only marks grammatical relations through word order. [183] The subject constituent precedes the verb and the object constituent follows it. The example below demonstrates how the grammatical roles of each constituent is marked only by the position relative to the verb:
An exception is found in sentences where one of the constituents is a pronoun, in which case it is doubly marked, both by word order and by case inflection, where the subject pronoun precedes the verb and takes the subjective case form, and the object pronoun follows the verb and takes the objective case form. The example below demonstrates this double marking in a sentence where both object and subject is represented with a third person singular masculine pronoun:
Indirect objects (IO) of ditransitive verbs can be placed either as the first object in a double object construction (S V IO O), such as I gave Jane the book or in a prepositional phrase, such as I gave the book to Jane [184]

Clause syntax
In English a sentence may be composed of one or more clauses, that may in turn be composed of one or more phrases (e.g. Noun Phrases, Verb Phrases, and Prepositional Phrases). A clause is built around a verb, and includes its constituents, such as any NPs and PPs. Within a sentence one clause is always the main clause (or matrix clause) whereas other clauses are subordinate to it. Subordinate clauses may function as arguments of the verb in the main clause. For example, in the phrase I think (that) you are lying , the main clause is headed by the verb think , the subject is I , but the object of the phrase is the subordinate clause (that) you are lying . The subordinating conjunction that shows that the clause that follows is a subordinate clause, but it is often omitted. [185] Relative clauses are clauses that function as a modifier or specifier to some constituent in the main clause: For example, in the sentence I saw the letter that you received today , the relative clause that you received today specifies the meaning of the word letter , the object of the main clause. Relative clauses can be introduced by the pronouns who , whose , whom and which as well as by that (which can also be omitted.) [186] In contrast to many other Germanic languages there is no major differences between word order in main and subordinate clauses. [187]

Auxiliary verb constructions
English syntax relies on auxiliary verbs for many functions including the expression of tense, aspect and mood. Auxiliary verbs form main clauses, and the main verbs function as heads of a subordinate clause of the auxiliary verb. For example, in the sentence the dog did not find its bone , the clause find its bone is the complement of the negated verb did not . Subject–auxiliary inversion is used in many constructions, including focus, negation, and interrogative constructions.
The verb do can be used as an auxiliary even in simple declarative sentences, where it usually serves to add emphasis, as in "I did shut the fridge." However, in the negated and inverted clauses referred to above, it is used because the rules of English syntax permit these constructions only when an auxiliary is present. Modern English does not allow the addition of the negating adverb not to an ordinary finite lexical verb, as in *I know not —it can only be added to an auxiliary (or copular ) verb, hence if there is no other auxiliary present when negation is required, the auxiliary do is used, to produce a form like I do not (don't) know. The same applies in clauses requiring inversion, including most questions—inversion must involve the subject and an auxiliary verb, so it is not possible to say *Know you him? ; grammatical rules require Do you know him? [188]
Negation is done with the adverb not , which precedes the main verb and follows an auxiliary verb. A contracted form of not -n't can be used as an enclitic attaching to auxiliary verbs and to the copula verb to be . Just as with questions, many negative constructions require the negation to occur with do-support, thus in Modern English I don't know him is the correct answer to the question Do you know him? , but not *I know him not , although this construction may be found in older English. [189]
Passive constructions also use auxiliary verbs. A passive construction rephrases an active construction in such a way that the object of the active phrase becomes the subject of the passive phrase, and the subject of the active phrase is either omitted or demoted to a role as an oblique argument introduced in a prepositional phrase. They are formed by using the past participle either with the auxiliary verb to be or to get , although not all varieties of English allow the use of passives with get . For example, putting the sentence she sees him into the passive becomes he is seen (by her) , or he gets seen (by her) . [190]

Questions
Both yes–no questions and wh -questions in English are mostly formed using subject–auxiliary inversion ( Am I going tomorrow? , Where can we eat? ), which may require do -support ( Do you like her? , Where did he go? ). In most cases, interrogative words ( wh -words; e.g. what , who , where , when , why , how ) appear in a fronted position . For example, in the question What did you see? , the word what appears as the first constituent despite being the grammatical object of the sentence. (When the wh -word is the subject or forms part of the subject, no inversion occurs: Who saw the cat? .) Prepositional phrases can also be fronted when they are the question's theme, e.g. To whose house did you go last night? . The personal interrogative pronoun who is the only interrogative pronoun to still show inflection for case, with the variant whom serving as the objective case form, although this form may be going out of use in many contexts. [191]

Discourse level syntax
At the discourse level English tends to use a topic-comment structure, where the known information (topic) precedes the new information (comment). Because of the strict SVO syntax, the topic of a sentence generally has to be the grammatical subject of the sentence. In cases where the topic is not the grammatical subject of the sentence, frequently the topic is promoted to subject position through syntactic means. One way of doing this is through a passive construction, the girl was stung by the bee . Another way is through a cleft sentence where the main clause is demoted to be a complement clause of a copula sentence with a dummy subject such as it or there , e.g. it was the girl that the bee stung , there was a girl who was stung by a bee . [192] Dummy subjects are also used in constructions where there is no grammatical subject such as with impersonal verbs (e.g., it is raining ) or in existential clauses ( there are many cars on the street ). Through the use of these complex sentence constructions with informationally vacuous subjects, English is able to maintain both a topic comment sentence structure and a SVO syntax.
Focus constructions emphasise a particular piece of new or salient information within a sentence, generally through allocating the main sentence level stress on the focal constituent. For example, the girl was stung by a bee (emphasising it was a bee and not for example a wasp that stung her), or The girl was stung by a bee (contrasting with another possibility, for example that it was the boy). [193] Topic and focus can also be established through syntactic dislocation, either preposing or postposing the item to be focused on relative to the main clause. For example, That girl over there, she was stung by a bee , emphasises the girl by preposition, but a similar effect could be achieved by postposition, she was stung by a bee, that girl over there , where reference to the girl is established as an "afterthought". [194]
Cohesion between sentences is achieved through the use of deictic pronouns as anaphora (e.g. that is exactly what I mean where that refers to some fact known to both interlocutors, or then used to locate the time of a narrated event relative to the time of a previously narrated event). [195] Discourse markers such as oh , so or well , also signal the progression of ideas between sentences and help to create cohesion. Discourse markers are often the first constituents in sentences. Discourse markers are also used for stance taking in which speakers position themselves in a specific attitude towards what is being said, for example, no way is that true! (the idiomatic marker no way! expressing disbelief), or boy! I'm hungry (the marker boy expressing emphasis). While discourse markers are particularly characteristic of informal and spoken registers of English, they are also used in written and formal registers. [196]

Vocabulary
The vocabulary of English is vast, and counting exactly how many words English (or any language) has is impossible. [197] [198] [199] The Oxford Dictionaries suggest that there are at least a quarter of a million distinct English words. [197] Early studies of English vocabulary by lexicographers , the scholars who formally study vocabulary, compile dictionaries, or both, were impeded by a lack of comprehensive data on actual vocabulary in use from good-quality linguistic corpora , [200] collections of actual written texts and spoken passages. Many statements published before the end of the 20th century about the growth of English vocabulary over time, the dates of first use of various words in English, and the sources of English vocabulary will have to be corrected as new computerised analysis of linguistic corpus data becomes available. [199] [201]

Word formation processes
English forms new words from existing words or roots in its vocabulary through a variety of processes. One of the most productive processes in English is conversion, [202] using a word with a different grammatical role, for example using a noun as a verb or a verb as a noun. Another productive word-formation process is nominal compounding, [199] [201] producing compound words such as babysitter or ice cream or homesick . [202] A process more common in Old English than in Modern English, but still productive in Modern English, is the use of derivational suffixes ( -hood , -ness , -ing , -ility ) to derive new words from existing words (especially those of Germanic origin) or stems (especially for words of Latin or Greek origin). Formation of new words, called neologisms , based on Greek or Latin roots (for example television or optometry ) is a highly productive process in English and in most modern European languages, so much so that it is often difficult to determine in which language a neologism originated. For this reason, lexicographer Philip Gove attributed many such words to the " international scientific vocabulary " (ISV) when compiling Webster's Third New International Dictionary (1961). Another active word-formation process in English is acronyms , [203] words formed by pronouncing as a single word abbreviations of longer phrases (e.g. NATO , laser ).

Word origins
English, besides forming new words from existing words and their roots, also borrows words from other languages. This process of adding words from other languages is commonplace in many world languages, but English is characterised as being especially open to borrowing of foreign words throughout the last 1,000 years. [205] The most commonly used words in English are West Germanic. [206] The words in English learned first by children as they learn to speak, particularly the grammatical words that dominate the word count of both spoken and written texts, are the Germanic words inherited from the earliest periods of the development of Old English. [199] But one of the consequences of long language contact between French and English in all stages of their development is that the vocabulary of English has a very high percentage of "Latinate" words (derived from French, especially, and also from Latin or from other Romance languages). [207] French words from various periods of the development of French now make up one-third of the vocabulary of English. [208]
English has also borrowed many words directly from Latin, the ancestor of the Romance languages, during all stages of its development. [201] [199] Many of these words were earlier borrowed into Latin from Greek. Latin or Greek are still highly productive sources of stems used to form vocabulary of subjects learned in higher education such as the sciences, philosophy, and mathematics. [209] English continues to gain new loanwords and calques ("loan translations") from languages all over the world, and words from languages other than the ancestral Anglo-Saxon language make up about 60 percent of the vocabulary of English. [210] English has formal and informal speech registers , and informal registers, including child directed speech, tend to be made up predominantly of words of Anglo-Saxon origin, while the percentage of vocabulary that is of Latinate origin is higher in legal, scientific, and academic texts. [211] [212]

English loanwords and calques in other languages
English has a strong influence on the vocabulary of other languages. [208] [213] The influence of English comes from such factors as opinion leaders in other countries knowing the English language, the role of English as a world lingua franca, and the large number of books and films that are translated from English into other languages. [214] That pervasive use of English leads to a conclusion in many places that English is an especially suitable language for expressing new ideas or describing new technologies. Among varieties of English, it is especially American English that influences other languages. [215] Some languages, such as Chinese, write words borrowed from English mostly as calques , while others, such as Japanese, readily take in English loanwords written in sound-indicating script. [216] Dubbed films and television programmes are an especially fruitful source of English influence on languages in Europe. [216]

Writing system
Since the ninth century, English has been written in a Latin alphabet (also called Roman alphabet). Earlier Old English texts in Anglo-Saxon runes are only short inscriptions. The great majority of literary works in Old English that survive to today are written in the Roman alphabet. [28] The modern English alphabet contains 26 letters of the Latin script : a , b , c , d , e , f , g , h , i , j , k , l , m , n , o , p , q , r , s , t , u , v , w , x , y , z (which also have capital forms: A, B, C, D, E, F, G, H, I, J, K, L, M, N, O, P, Q, R, S, T, U, V, W, X, Y, Z).
The spelling system, or orthography , of English is multi-layered, with elements of French, Latin, and Greek spelling on top of the native Germanic system. [217] Further complications have arisen through sound changes with which the orthography has not kept pace. [40] Compared to European languages for which official organisations have promoted spelling reforms, English has spelling that is a less consistent indicator of pronunciation and standard spellings of words that are more difficult to guess from knowing how a word is pronounced. [218] There are also systematic spelling differences between British and American English . These situations have prompted proposals for spelling reform in English. [219]
Although letters and speech sounds do not have a one-to-one correspondence in standard English spelling, spelling rules that take into account syllable structure, phonetic changes in derived words, and word accent are reliable for most English words. [220] Moreover, standard English spelling shows etymological relationships between related words that would be obscured by a closer correspondence between pronunciation and spelling, for example the words photograph , photography , and photographic , [220] or the words electricity and electrical . While few scholars agree with Chomsky and Halle (1968) that conventional English orthography is "near-optimal", [217] there is a rationale for current English spelling patterns. [221] The standard orthography of English is the most widely used writing system in the world. [222] Standard English spelling is based on a graphomorphemic segmentation of words into written clues of what meaningful units make up each word. [223]
Readers of English can generally rely on the correspondence between spelling and pronunciation to be fairly regular for letters or digraphs used to spell consonant sounds. The letters b, d, f, h, j, k, l, m, n, p, r, s, t, v, w, y, z represent, respectively, the phonemes /b, d, f, h, dʒ, k, l, m, n, p, r, s, t, v, w, j, z/ . The letters c and g normally represent /k/ and /ɡ/ , but there is also a soft c pronounced /s/ , and a soft g pronounced /dʒ/ . The differences in the pronunciations of the letters c and g are often signalled by the following letters in standard English spelling. Digraphs used to represent phonemes and phoneme sequences include ch for /tʃ/ , sh for /ʃ/ , th for /θ/ or /ð/ , ng for /ŋ/ , qu for /kw/ , and ph for /f/ in Greek-derived words. The single letter x is generally pronounced as /z/ in word-initial position and as /ks/ otherwise. There are exceptions to these generalisations, often the result of loanwords being spelled according to the spelling patterns of their languages of origin [220] or proposals by pedantic scholars in the early period of Modern English to mistakenly follow the spelling patterns of Latin for English words of Germanic origin. [224]
For the vowel sounds of the English language, however, correspondences between spelling and pronunciation are more irregular. There are many more vowel phonemes in English than there are vowel letters ( a , e , i , o , u , w , y ). As a result of a smaller set of single letter symbols than the set of vowel phonemes, some " long vowels " are often indicated by combinations of letters (like the oa in boat , the ow in how , and the ay in stay ), or the historically based silent e (as in note and cake ). [221]
The consequence of this complex orthographic history is that learning to read can be challenging in English. It can take longer for school pupils to become independently fluent readers of English than of many other languages, including Italian, Spanish, or German. [225] Nonetheless, there is an advantage for learners of English reading in learning the specific sound-symbol regularities that occur in the standard English spellings of commonly used words. [220] Such instruction greatly reduces the risk of children experiencing reading difficulties in English. [226] [227] Making primary school teachers more aware of the primacy of morpheme representation in English may help learners learn more efficiently to read and write English. [228]
English writing also includes a system of punctuation that is similar to the system of punctuation marks used in most alphabetic languages around the world. The purpose of punctuation is to mark meaningful grammatical relationships in sentences to aid readers in understanding a text and to indicate features important for reading a text aloud. [229]

Dialects, accents, and varieties
Dialectologists distinguish between English dialects , regional varieties that differ from each other in terms of grammar and vocabulary, and regional accents , distinguished by different patterns of pronunciation. The major native dialects of English are often divided by linguists into the two general categories of the British dialects (BrE) and those of North America (AmE). [230] There also exists a grouping of major native varieties of English in the southern hemisphere, the most prominent being Australian and New Zealand English .

UK and Ireland
As the place where English first evolved, the British Isles, and particularly England, are home to the most variegated pattern of dialects. Within the United Kingdom, the Received Pronunciation (RP), an educated dialect of South East England , is traditionally used as the broadcast standard, and is considered the most prestigious of the British dialects. The spread of RP (also known as BBC English) through the media has caused many traditional dialects of rural England to recede, as youths adopt the traits of the prestige variety instead of traits from local dialects. At the time of the Survey of English Dialects , grammar and vocabulary differed across the country, but a process of lexical attrition has led most of this variation to disappear. [231] Nonetheless this attrition has mostly affected dialectal variation in grammar and vocabulary, and in fact only 3 percent of the English population actually speak RP, the remainder speaking regional accents and dialects with varying degrees of RP influence. [232] There is also variability within RP, particularly along class lines between Upper and Middle class RP speakers and between native RP speakers and speakers who adopt RP later in life. [233] Within Britain there is also considerable variation along lines of social class, and some traits though exceedingly common are considered "non-standard" and are associated with lower class speakers and identities. An example of this is H-dropping , which was historically a feature of lower class London English, particularly Cockney, but which today is the standard in all major English cities—yet it remains largely absent in broadcasting and among the upper crust of British society. [234]
English in England can be divided into four major dialect regions, Southwest English , South East English, Midlands English, and Northern English . Within each of these regions several local subdialects exist: Within the Northern region, there is a division between the Yorkshire dialects, and the Geordie dialect spoken in Northumbria around Newcastle, and the Lancashire dialects with local urban dialects in Liverpool ( Scouse ) and Manchester ( Mancunian ). Having been the centre of Danish occupation during the Viking Invasions, Northern English dialects, particularly the Yorkshire dialect, retain Norse features not found in other English varieties. [235]
Since the 15th century, Southeastern varieties centred around London, which has been the centre from which dialectal innovations have spread to other dialects. In London, the Cockney dialect was traditionally used by the lower classes, and it was long a socially stigmatised variety. Today a large area of Southeastern England has adopted traits from Cockney, resulting in the so-called Estuary English which spread in areas south and East of London beginning in the 1980s. Estuary English is distinguished by traits such as the use of intrusive R ( drawing is pronounced drawring /ˈdrɔːrɪŋ/ ), t -glottalisation ( Potter is pronounced with a glottal stop as Po'er /poʔʌ/ ), and the pronunciation of th- as /f/ ( thanks pronounced fanks ) or /v/ ( bother pronounced bover ). [236]
Scots is today considered a separate language from English, but it has its origins in early Northern Middle English [237] and developed and changed during its history with influence from other sources, particularly Scots Gaelic and Old Norse. Scots itself has a number of regional dialects. And in addition to Scots, Scottish English are the varieties of Standard English spoken in Scotland, most varieties are Northern English accents, with some influence from Scots. [238]
In Ireland , various forms of English have been spoken since the Norman invasions of the 11th century. In County Wexford , in the area surrounding Dublin , two highly conservative dialects known as Forth and Bargy and Fingallian developed as offshoots from Early Middle English, and were spoken until the 19th century. Modern Hiberno-English however has its roots in English colonisation in the 17th century. Today Irish English is divided into Ulster English, a dialect with strong influence from Scots, and southern Hiberno-English. Like Scots and Northern English, the Irish accents preserve the rhoticity which has been lost in most dialects influenced by RP. [15] [239]

North America
American English is fairly homogeneous compared to British English. Today, American accent variation is often increasing at the regional level and decreasing at the very local level, [240] though most Americans still speak within a phonological continuum of similar accents, [241] known collectively as General American (GA), with differences hardly noticed even among Americans themselves (such as Midland and Western American English ). [242] [243] [244] In most American and Canadian English, rhoticity (or r -fulness) is dominant, with non-rhoticity ( r -dropping) becoming associated with lower prestige and social class especially after World War II; this contrasts with the situation in England, where non-rhoticity has become the standard. [245] Separate from GA are American dialects with clearly distinct sound systems, historically including Southern American English , English of the coastal Northeast (famously including Eastern New England English and New York City English ), and African American Vernacular English , all of which are historically non-rhotic. Canadian English , except for the Atlantic provinces and perhaps Quebec , may be classified under GA as well, but it often shows raising of certain vowels , / aɪ / and / aʊ / , before voiceless consonants , as well as distinct norms for written and pronunciation standards. [246]
In Southern American English , the largest American "accent group" outside of GA, [247] rhoticity now strongly prevails, replacing the region's historical non-rhotic prestige. [248] [249] [250] Southern accents are colloquially described as a "drawl" or "twang," [251] being recognised most readily by the Southern Vowel Shift that begins with glide-deleting in the /aɪ/ vowel (e.g. pronouncing spy almost like spa ), the "Southern breaking" of several front pure vowels into a gliding vowel or even two syllables (e.g. pronouncing the word "press" almost like "pray-us"), [252] the pin–pen merger , and other distinctive phonological, grammatical, and lexical features, many of which are actually recent developments of the 19th century or later. [253]
Today spoken primarily by working- and middle-class African Americans , African American Vernacular English (AAVE) is also largely non-rhotic and likely originated among enslaved Africans and African Americans influenced primarily by the non-rhotic, non-standard English dialects of the Old South . A minority of linguists, [254] contrarily, propose that AAVE mostly traces back to African languages spoken by the slaves who had to develop a pidgin or Creole English to communicate with slaves of other ethnic and linguistic origins. [255] AAVE shares important commonalities with older Southern American English and so probably developed to a highly coherent and homogeneous variety in the 19th or early 20th century. AAVE is commonly stigmatised in North America as a form of "broken" or "uneducated" English, also common of modern Southern American English, but linguists today recognise both as fully developed varieties of English with their own norms shared by a large speech community. [256] [257]

Australia and New Zealand
Since 1788, English has been spoken in Oceania , and Australian English has developed as a first language of the vast majority of the inhabitants of the Australian continent, its standard accent being General Australian . The English of neighbouring New Zealand has to a lesser degree become an influential standard variety of the language. [258] Australian and New Zealand English are most closely related to each other with few differentiating characteristics, followed by South African English and the English of southeastern England, all of which have similarly non-rhotic accents, aside from some accents in the South Island of New Zealand. Australian and New Zealand English stand out for their innovative vowels: many short vowels are fronted or raised, whereas many long vowels have diphthongised. Australian English also has a contrast between long and short vowels, not found in most other varieties. Australian English grammar aligns closely to British and American English; like American English, collective plural subjects take on a singular verb (as in the government is rather than are ). [259] [260] New Zealand English uses [ʍ] for ⟨wh-⟩ and its front vowels are often even higher than in Australian English. [261] [262] [263]

Africa, the Caribbean, and South Asia
English is spoken widely in South Africa and is an official or co-official language in several countries. In South Africa , English has been spoken since 1820, co-existing with Afrikaans and various African languages such as the Khoe and Bantu languages . Today about 9 percent of the South African population speak South African English (SAE) as a first language. SAE is a non-rhotic variety, which tends to follow RP as a norm. It is alone among non-rhotic varieties in lacking intrusive r. There are different L2 varieties that differ based on the native language of the speakers. [264] Most phonological differences from RP are in the vowels. [265] Consonant differences include the tendency to pronounce /p, t, t͡ʃ, k/ without aspiration (e.g. pin pronounced [pɪn] rather than as [pʰɪn] as in most other varieties), while r is often pronounced as a flap [ɾ] instead of as the more common fricative. [266]
Several varieties of English are also spoken in the Caribbean Islands that were colonial possessions of Britain, including Jamaica, and the Leeward and Windward Islands and Trinidad and Tobago , Barbados , the Cayman Islands , and Belize . Each of these areas are home both to a local variety of English and a local English based creole, combining English and African languages. The most prominent varieties are Jamaican English and Jamaican Creole . In Central America, English based creoles are spoken in on the Caribbean coasts of Nicaragua and Panama. [267] Locals are often fluent both in the local English variety and the local creole languages and code-switching between them is frequent, indeed another way to conceptualise the relationship between Creole and Standard varieties is to see a spectrum of social registers with the Creole forms serving as "basilect" and the more RP-like forms serving as the "acrolect", the most formal register. [268]
Most Caribbean varieties are based on British English and consequently most are non-rhotic, except for formal styles of Jamaican English which are often rhotic. Jamaican English differs from RP in its vowel inventory, which has a distinction between long and short vowels rather than tense and lax vowels as in Standard English. The diphthongs /ei/ and /ou/ are monophthongs [eː] and [oː] or even the reverse diphthongs [ie] and [uo] (e.g. bay and boat pronounced [bʲeː] and [bʷoːt] ). Often word final consonant clusters are simplified so that "child" is pronounced [t͡ʃail] and "wind" [win] . [269] [270] [271]
As a historical legacy, Indian English tends to take RP as its ideal, and how well this ideal is realised in an individual's speech reflects class distinctions among Indian English speakers. Indian English accents are marked by the pronunciation of phonemes such as /t/ and /d/ (often pronounced with retroflex articulation as [ʈ] and [ɖ] ) and the replacement of /θ/ and /ð/ with dentals [t̪] and [d̪] . Sometimes Indian English speakers may also use spelling based pronunciations where the silent ⟨h⟩ found in words such as ghost is pronounced as an Indian voiced aspirated stop [ɡʱ] . [272]
WebPage index: 00003
First-person shooter
First-person shooter ( FPS ) is a video game genre centered around gun and other weapon-based combat in a first-person perspective ; that is, the player experiences the action through the eyes of the protagonist . The genre shares common traits with other shooter games , which in turn makes it fall under the heading action game . Since the genre's inception, advanced 3D and pseudo-3D graphics have challenged hardware development, and multiplayer gaming has been integral.
The first-person shooter genre has been traced as far back as Maze War , development of which began in 1973, and 1974's Spasim . Later on and after more playful titles like MIDI Maze in 1987, the genre coalesced into a more wantonly violent form with 1992's Wolfenstein 3D , which has been credited with creating the genre's basic archetype, that subsequent titles were based upon. One such title, and the progenitor of the genre's wider mainstream acceptance and popularity was Doom , perhaps one of the most influential games in this genre; for many years, the term Doom clone was used to designate this genre due to Doom ' s influence. 1998's Half-Life —along with its 2004 sequel Half-Life 2 —enhanced the narrative and puzzle elements. [1] [2] In 1999, the Half-Life ' s mod Counter-Strike was released and, together with Doom , is perhaps one of the most influential first-person shooters. GoldenEye 007 , released in 1997, was a landmark first-person shooter for home consoles , while the Halo series heightened the console's commercial and critical appeal as a platform for first-person shooter titles. In the 21st century, the first-person shooter is the most commercially viable video game genre, and in 2016, shooters accounted for over 27% of all video game genres. [3] Several first-person shooters have been popular games for eSports and competitive gaming competitions as well.

Definition
First-person shooters are a type of three-dimensional shooter game , [4] featuring a first-person point of view with which the player sees the action through the eyes of the player character . They are unlike third-person shooters , in which the player can see (usually from behind) the character they are controlling. The primary design element is combat, mainly involving firearms. [5]
First person-shooter games are also often categorized as being distinct from light gun shooters , a similar genre with a first-person perspective which use light gun peripherals, in contrast to first-person shooters which use conventional input devices for movement. [6] A more important key difference is that first-person light-gun shooters like Virtua Cop often feature "on-rails" movement, whereas first-person shooters like Doom give the player more freedom to roam.
The first-person shooter may be considered a distinct genre in itself, or a type of shooter game, in turn a subgenre of the wider action game genre. [7] Following the release of Doom in 1993, games in this style were commonly termed "Doom clones"; [8] [9] in time this term has largely been replaced by "first-person shooter". [9] Wolfenstein 3D, released in 1992, the year before Doom , has been credited with inventing the genre, but critics have since identified similar though less advanced games developed as far back as 1973. [5] There are occasional disagreements regarding the specific design elements which constitute a first-person shooter. For example, Deus Ex or BioShock may be considered as first-person shooters, but may also be considered role-playing video games as they borrow from this genre extensively. [10] Some commentators extend the definition to include combat flight simulators where the cockpit or vehicle takes place of the hands and weapons. [1] [5]

Game design
Like most shooter games, first-person shooters involve an avatar , one or more ranged weapons , and a varying number of enemies. [7] Because they take place in a 3D environment, these games tend to be somewhat more realistic than 2D shooter games, and have more accurate representations of gravity, lighting, sound and collisions. [4] First-person shooters played on personal computers are most often controlled with a combination of a keyboard and mouse . This system has been claimed as superior to that found in console games, [11] [12] which frequently use two analog sticks : one used for running and sidestepping, the other for looking and aiming . [13] It is common to display the character's hands and weaponry in the main view, with a head-up display showing health, ammunition and location details. Often, it is possible to overlay a map of the surrounding area. [14]

Combat and power-ups
First-person shooters often focus on action gameplay, with fast-paced and bloody firefights, though some place a greater emphasis on narrative, problem-solving and logic puzzles. [15] In addition to shooting, melee combat may also be used extensively. In some games, melee weapons are especially powerful, a reward for the risk the player must take in maneuvering his character into close proximity to the enemy. [16] In other situations, a melee weapon may be less effective, but necessary as a last resort. [17] " Tactical shooters " are more realistic, and require teamwork and strategy to succeed; [13] the player often commands a squad of characters, which may be controlled by the game or by human teammates. [18]
First-person shooters typically give players a choice of weapons, which have a large impact on how the player will approach the game. [4] Some game designs have realistic models of actual existing or historical weapons, incorporating their rate of fire, magazine size, ammunition amount, recoil and accuracy. Other first-person shooter games may incorporate imaginative variations of weapons, including future prototypes, "alien technology" scenario defined weaponry, and/or utilizing a wide array of projectiles, from industrial labor tools to laser, energy, plasma, rocket and grenade launchers or crossbows. These many variations may also be applied to the tossing animations of grenades, rocks, spears and the such. Also more unconventional modes of destruction may be employed from the viewable users hands such as flames, electricity, telekinesis or other supernatural constructions. However, designers often allow characters to carry varying multiples of weapons with little to no reduction in speed or mobility, or perhaps more realistically, a pistol or smaller device and a long rifle or even limiting the player to only one weapon at a time. There are often options to trade up, upgrade or swap out in most games. Thus, the standards of realism varies between design elements. [19] The protagonist can generally be healed and re-armed by means of items such as first aid kits , simply by walking over them. [20] Some games allow players to accumulate experience points similar to those found in role-playing games, which can unlock new weapons and abilities. [21]

Level design
First-person shooters may be structurally composed of levels , or use the technique of a continuous narrative in which the game never leaves the first-person perspective. [1] Others feature large sandbox environments, which are not divided into levels and can be explored freely. [22] In first-person shooters, protagonists interact with the environment to varying degrees, from basics such as using doors, to problem solving puzzles based on a variety of interactive objects. [1] In some games, the player can damage the environment, also to varying degrees: one common device is the use of barrels containing explosive material which the player can shoot, destroying them and harming nearby enemies. [20] Other games feature environments which are extensively destructible, allowing for additional visual effects. [23] The game world will often make use of science fiction, historic (particularly World War II ) or modern military themes, with such antagonists as aliens , monsters, terrorists and soldiers of various types. [24] Games feature multiple difficulty settings; in harder modes, enemies are tougher, more aggressive and do more damage, and power-ups are limited. In easier modes, the player can succeed through reaction times alone; on more difficult settings, it is often necessary to memorize the levels through trial and error. [25]

Multiplayer
First-person shooters may feature a multiplayer mode, taking place on specialized levels. Some games are designed specifically for multiplayer gaming, and have very limited single player modes in which the player competes against game-controlled characters termed "bots". [26] Massively multiplayer online first-person shooters allow thousands of players to compete at once in a persistent world . [27] Large scale multiplayer games allow multiple squads, with leaders issuing commands and a commander controlling the team's overall strategy. [26] Multiplayer games have a variety of different styles of match.
The classic types are the deathmatch (and its team-based variant) in which players score points by killing other players' characters; and capture the flag , in which teams attempt to penetrate the opposing base, capture a flag and return it to their own base whilst preventing the other team from doing the same. Other game modes may involve attempting to capture enemy bases or areas of the map, attempting to take hold of an object for as long as possible while evading other players, or deathmatch variations involving limited lives or in which players fight over a particularly potent power-up . These match types may also be customizable, allowing the players to vary weapons, health and power-ups found on the map, as well as victory criteria. [28] Games may allow players to choose between various classes , each with its own strengths, weaknesses, equipment and roles within a team. [17]

History

Origins: 1970s to late 1980s
The earliest two documented first-person shooter video games are Maze War and Spasim . Maze War features on-foot gameplay that evokes modern first-person shooter games. Development of the game began in 1973 and its exact date of completion is unknown. Spasim had a documented debut at the University of Illinois in 1974. The game was a rudimentary space flight simulator , which featured a first-person perspective. [5] They were distinct from modern first-person shooters, involving simple tile-based movement where the player could only move from square to square and turn in 90-degree increments. [29] Spasim led to more detailed combat flight simulators and eventually to a tank simulator , developed for the U.S. Army , in the later 1970s. These games were not available to consumers, however, and it was not until 1980 that a tank video game, Battlezone , was released in arcades . A version of the game was released in 1983 for home computers and became the first successful mass-market game featuring a first-person viewpoint and wireframe 3D graphics , [30] presented using a vector graphics display . [31]

Early first-person shooters: 1987–1992
MIDI Maze , an early first-person shooter released in 1987 for the Atari ST , [32] featured maze-based gameplay and character designs similar to Pac-Man , but displayed in a first-person perspective. [33] [34] Later ported to various systems - including the Game Boy and Super NES - under the title Faceball 2000 , [35] it featured the first network multiplayer deathmatches , using a MIDI interface. [34] It was a relatively minor game, but despite the inconvenience of connecting numerous machines together, its multiplayer mode gained a cult following: 1UP.com called it the "first multi-player 3D shooter on a mainstream system" and the first "major LAN action game". [35]
Id Software's Hovertank 3D pioneered ray casting technology in May 1991 to enable faster gameplay than 1980s vehicle simulators; [30] and six months later Catacomb 3-D introduced another advance, texture mapping , in November 1991. The second game to use texture mapping was Ultima Underworld: The Stygian Abyss , a March 1992 action role-playing game by Looking Glass Technologies that featured a first-person viewpoint and an advanced graphics engine. In October 1990, id developer John Romero learned about texture mapping from a phone call to Paul Neurath. Romero described the texture mapping technique to id programmer John Carmack , who remarked, "I can do that.", [36] and would feel motivated by Looking Glass's example to do the same in Catacomb 3-D . [30] Catacomb 3-D also introduced the display of the protagonist's hand and weapon (in this case, magical spells) on the screen, whereas previously aspects of the player's avatar were not visible. [30] The experience of developing Ultima Underworld would make it possible for Looking Glass to create the Thief and System Shock series years later. [37]

Rise in popularity: 1992–1995
Wolfenstein 3D (created by id Software and released in 1992) was an instant success, fueled largely by its shareware release, and has been credited with inventing the first-person shooter genre. [1] [5] [38] It was built on the ray casting technology pioneered in earlier games to create a revolutionary template for shooter game design, which first-person shooters are still based upon today. [1] [5] [15] Despite its violent themes, Wolfenstein largely escaped the controversy generated by the later Doom , although it was banned in Germany due to the use of Nazi iconography ; [39] and the Super NES version replaced the enemy attack dogs with giant rats . [40] Apogee Software , the publisher of Wolfenstein 3D, followed up its success with Blake Stone: Aliens of Gold in 1993. The game was initially well-received but sales rapidly declined in the wake of the success of id's Doom , released a week later. [41]
Doom , released as shareware in 1993, [15] refined Wolfenstein 3D's template by adding improved textures, variations in height (e.g., stairs the player's character could climb) and effects such as flickering lights and patches of total darkness, creating a more believable 3D environment than Wolfenstein 3D's more monotonous and simplistic levels. [42] Doom allowed competitive matches between multiple players, termed "deathmatches," and the game was responsible for the word's subsequent entry into the video gaming lexicon. [42] According to creator John Romero , the game's deathmatch concept was inspired by the competitive multiplayer of fighting games . [43] Doom became so popular that its multiplayer features began to cause problems for companies whose networks were used to play the game. [15] [42]
Doom has been considered the most important first-person shooter ever made. It was highly influential not only on subsequent shooter games but on video gaming in general, [42] and has been available on almost every video gaming system since. [15] Multiplayer gaming, which is now integral to the first-person shooter genre, was first achieved successfully on a large scale with Doom . [1] [42] While its combination of gory violence , dark humor and hellish imagery garnered acclaim from critics, [42] [44] these attributes also generated criticism from religious groups, with other commentators labelling the game a "murder simulator." [45] There was further controversy when it emerged that the perpetrators of the Columbine High School massacre were fans of the game; the families of several victims later unsuccessfully attempted to sue numerous video game companies - among them id Software - which the families claimed inspired the massacre. [39]
In 1994, Raven Software released Heretic , which used a modified version of the Doom engine that allowed for vertical aiming, an inventory system to store and select items, and gibs . On the Macintosh , Bungie 's release, in the same year, of Marathon , and its subsequent sequels, set the standard for first-person shooters on that platform. Marathon pioneered or was an early adopter of several new features such as freelook , dual-wielded and dual-function weapons, versatile multiplayer modes (such as King of the Hill, Kill the Man with the Ball, and cooperative play), friendly NPCs , and a strong emphasis on storytelling in addition to the action. [46] Star Wars: Dark Forces was released in 1995 after LucasArts decided Star Wars would make appropriate material for a game in the style of Doom . However, Star Wars: Dark Forces added several technical features that Doom lacked, such as the ability to crouch or look up and down, [8] [15] [47] Apogee's Duke Nukem 3D , released in 1996, was "the last of the great, sprite-based shooters" [15] winning acclaim for its humor based around stylised machismo as well as its gameplay. However, some found the game's (and later the whole series') treatment of women to be derogatory and tasteless. [15] [39] [48]

Advances in 3D graphics: 1995–1999
In 1994, Exact released Geograph Seal for the Japanese Sharp X68000 home computer. An obscure import title as far as the Western market was concerned, it was nonetheless "a fully 3D polygonal first-person shooter" with innovative platform game mechanics and "free-roaming" outdoor environments. The following year, Exact released its successor for the PlayStation console, Jumping Flash! , which placed more emphasis on its platform elements. [49] Descent (released by Parallax Software in 1995), a game in which the player pilots a spacecraft around caves and factory ducts, was a truly three-dimensional first-person shooter. It abandoned sprites and ray casting in favour of polygons and six degrees of freedom . [1] [15]
Shortly after the release of Duke Nukem 3D in 1996, id Software released the much anticipated Quake . Like Doom, Quake was influential and genre-defining, featuring fast-paced, gory gameplay, but used 3D polygons instead of sprites. It was centered on online gaming and featured multiple match types still found in first-person shooter games today. It was the first FPS game to have a following of player clans (although the concept had existed previously in MechWarrior 2 ( Netmech ) with its Battletech lore as well as amongst MUD players), and would inspire popular LAN parties such as QuakeCon . [50] The game's popularity and use of 3D polygonal graphics also helped to expand the growing market for video card hardware; [1] [15] [51] and the additional support and encouragement for game modifications attracted players who wanted to tinker with the game and create their own modules. [50] According to creator John Romero, Quake ' s 3D world was inspired by the 3D fighting game Virtua Fighter . Quake was also intended to expand the genre with Virtua Fighter influenced melee brawling , but this was eventually dropped from the final game. [52] [53]
Based on the James Bond film , Rare 's GoldenEye 007 was released in 1997, and as of 2004 it was the best-selling Nintendo 64 game in the United States. [54] It was the first landmark console first-person shooter and was highly acclaimed for its atmospheric single-player levels and well designed multiplayer maps. It featured a sniper rifle , the ability to perform head-shots, and the incorporation of stealth elements; [1] [15] [55] [56] (and all these aspects were also used in the game's spiritual sequel Perfect Dark ) as well as Virtua Cop -inspired features such as reloading, position-dependent hit reaction animations, penalties for killing innocents, and an aiming system allowing players to aim at a precise spot on the screen. [54]
Though not the first of its kind, Tom Clancy's Rainbow Six started a popular trend of tactical first-person shooters in 1998. It featured a team-based, realistic design and themes based around counter-terrorism , requiring missions to be planned before execution and in it, a single hit was sometimes enough to kill a character. [18] [57] Medal of Honor , released in 1999, started a long running proliferation of first-person shooters set during World War II. [15]
Valve's Half-Life was released in 1998, based upon Quake 's graphics technology. [58] Initially met with only mild anticipation, it went on to become an unprecedented commercial success. [15] [59] While previous first-person shooters had focused on visceral gameplay with comparatively weak plots, Half-Life had a strong narrative; the game featured no cut scenes but remained in the first-person perspective at all times. It featured innovations such as non-enemy characters (featured somewhat earlier in titles such as Strife ) [60] but did not employ power-ups in the traditional sense. [1] Half-Life was praised for its artificial intelligence , selection of weapons and attention to detail and "has since been recognized as one of the greatest games of all time" according to GameSpot. Its sequel Half-Life 2 (released in 2004), was less influential though "arguably a more impressive game". [61]
Starsiege: Tribes , also released in 1998, was a multiplayer online shooter allowing more than 32 players in a single match. It featured team-based gameplay with a variety of specialized roles, and an unusual jet pack feature. The game was highly popular and later imitated by games such as the Battlefield series. [1] [2] Id's Quake III Arena and Epic's Unreal Tournament , both released in 1999, were popular for their frenetic and accessible online multiplayer modes; both featured very limited single player gameplay. [15] Counter-Strike was also released in 1999, a Half-Life modification with a counter-terrorism theme. The game and later version Counter-Strike: Source (2004) went on to become by far the most popular multiplayer first-person shooter and computer game modification ever, with over 90,000 players competing online at any one time during its peak. [15] [58]

Online wars and return of the console: 2000–2006
At the E3 game show in 1999, Bungie unveiled a real-time strategy game called Halo ; at the following E3, an overhauled third-person shooter version was displayed. In 2000, Bungie was bought by Microsoft . Halo was then revamped and released as a first-person shooter; it was one of the launch titles for the Xbox console. It was a runaway critical and commercial success, and is considered a premier console first-person shooter. It featured narrative and storyline reminiscent of Bungie's earlier Marathon series but now told largely through in-game dialog and cut scenes. It also received acclaim for its characters, both the protagonist, Master Chief and its alien antagonists . The sequel, Halo 2 (2004), brought the popularity of online-gaming to the console market through the medium of Xbox Live , on which it was the most played game for almost two years. [15]
Deus Ex , released by Ion Storm in 2000, featured a levelling system similar to that found in role-playing games; it also had multiple narratives depending on how the player completed missions and won acclaim for its serious, artistic style. [15] The Resident Evil games Survivor in 2000 and Dead Aim in 2003 attempted to combine the light gun and first-person shooter genres along with survival horror elements. [62] Metroid Prime , released in 2002 for the Nintendo GameCube , a highly praised console first-person shooter, incorporated action adventure elements such as jumping puzzles and built on the Metroid series of 2D side-scrolling platform-adventures . [15] Taking a "massive stride forward for first-person games", the game emphasised its adventure elements rather than shooting and was credited by journalist Chris Kohler with "breaking the genre free from the clutches of Doom ". [63]
World War II Online , released in 2001, featured a persistent and "massively multiplayer environment", although IGN said that "the full realization of that environment is probably still a few years away." [64] Battlefield 1942 , another World War II shooter released in 2002, featured large scale battles incorporating aircraft, naval vessels, land vehicles and infantry combat. [15] In 2003, PlanetSide allowed hundreds of players at once to compete in a persistent world, [65] and was also promoted as the "world's first massively multiplayer online first person shooter." [27] Doom 3 , released in 2004, placed a greater emphasis on horror and frightening the player than previous games in the series and was a critically acclaimed best seller, [66] [67] though some commentators felt it lacked gameplay substance and innovation, putting too much emphasis on impressive graphics. [10] In 2005, a film based on Doom featured a sequence that emulated the viewpoint and action of the first-person shooter, but was critically derided as deliberately unintelligent and gratuitously violent. [68]
In 2005, F.E.A.R. was acclaimed [69] for successfully combining first-person shooter gameplay with a Japanese horror atmosphere. [70] Later in 2007, Irrational Games ' BioShock would be acclaimed by some commentators as the best game of that year for its innovation in artistry, narrative and design, [71] [72] [73] with some calling it the " spiritual successor " to Irrational's earlier System Shock 2 . [74] [75]
Finally, the Crytek games Far Cry (2004) and Crysis (2007) as well as Ubisoft 's Far Cry 2 (2008) would break new ground in terms of graphics and large, open-ended level design, [15] [76] whereas Call of Duty 4: Modern Warfare (2007), Resistance: Fall of Man (2006) and its sequel Resistance 2 (2008) presented increasingly refined linear levels and narratives, [77] with the fast pace and linearity of the Call of Duty games bearing a resemblance to rail shooters. [78] [79] In 2006, GamaSutra reported the first-person shooter as one of the biggest and fastest growing video game genres in terms of revenue for publishers. [80]

2007–present
In 2010, researchers at Leiden University showed that playing first-person shooter video games is associated with superior mental flexibility. Compared to non-players, players of such games were found to require a significantly shorter reaction time while switching between complex tasks, possibly because they are required to develop a more responsive mindset to rapidly react to fast-moving visual and auditory stimuli, and to shift back and forth between different sub-duties. [81] The use of motion detecting game controllers – particularly the Wii 's – "promised to make FPS controls more approachable and precise with an interface as simple as literally pointing to aim" and thus "dramatically reshape the first-person shooter." However, technical difficulties pertinent to functions other than aiming – such as maneuvering or reloading – prevented their widespread use among first-person shooters. [82] The Pointman user interface combines a motion-sensitive gamepad, head tracker and sliding foot pedals to increase the precision and level of control over one's avatar [83] in military first-person shooter games.

See also
WebPage index: 00004
3D Realms
3D Realms (legal name Apogee Software, Ltd. ) is an American video game publisher and video game developer based in Garland, Texas , United States, established in 1987. It is best known for popularizing the shareware distribution model and as the creator of franchises on the PC such as Duke Nukem , and also the publisher of other franchises such as Commander Keen and Wolfenstein 3D .
While the company is known as "3D Realms", the legal name of the company is Apogee Software, Ltd. The name "3D Realms" was initially created as a branding label in July 1994 for use by Apogee which would be dedicated to just 3D games (as Apogee was then known for several styles of games). However, shortly after this, 3D games started to dominate the industry, and Apogee decided to direct its focus on this style of game; as such, "Apogee" was abandoned as a trade name in late 1996 and 3D Realms Entertainment, Inc. was formed to continue the brand. [2] In July 2008, however, it announced that the brand Apogee Software would be revived with new games, but licensed to an external company, Apogee Software, LLC . [3]

History

Background
Apogee started in 1987 with the release of Scott Miller 's Kingdom of Kroz , which used crude extended ASCII characters as graphics. [4] Nevertheless, the game sold quite well and Apogee was born. In 1991, George Broussard joined the company as co-owner, bringing with him several games of his that were previously released under the name Micro F/X. [5]
Apogee published games by other developers in addition to its own in-house titles. One of these developers, id Software , contributed to Apogee's success with games such as Commander Keen and Wolfenstein 3D , but later severed their ties with Apogee with their release of Doom in 1993 (mostly because id was afraid Apogee could not handle the phone orders to upgrade the shareware version of Doom ). [6]

Shareware and the Apogee model
Unlike traditional larger publishers that sold games in retail outlets, Apogee (like many independent developers) sold their products using the shareware method, depending mostly on BBSes , such as Software Creations , for distribution of their software. President George Broussard explained, "We started marketing our games as shareware because it was an inexpensive way to get started." [7]
Initial Apogee games ( Beyond the Titanic and Supernova ) were distributed as traditional shareware; that is, giving away the full game for free, and asking the customer to pay for it if he/she liked it. Upon registering, the customer would be able to receive support and help for the game. However, this marketing model did not prove to be profitable enough, so Apogee decided to implement a variation on the shareware model. Starting with Kingdom of Kroz , Apogee would provide the first installment of a game composed of several episodes (usually three) for free (as shareware), and sell the remaining installments by mail order . Registering the first episode would also enable the customer to receive support for that game, as well as giving them cheat codes for it. This method became known as the Apogee Model . [8] Initially, each episode of a game was sold separately, with discounts for buying all the episodes together. Later games did not offer the option to buy a specific episode; the customer could play the shareware version (first episode) for free, and buy the full registered version (all episodes) if they liked the game. The former model has some similarities with the episodic model currently used by some game companies.
Apogee's commercial success led to the widespread adoption of the shareware model (and most of the time, the specific Apogee Model) by other major publishers such as Capstone , Parallax Software , id Software , [8] Activision and Epic MegaGames , and also led to a growth of Software Creations BBS, which would become the largest BBS in North America. Apogee later moved to the traditional retail model through distributors like GT Interactive ; however, it still offers its earlier titles via shareware.

Apogee to 3D Realms
With the original intent to create a division for every genre of game Apogee produced, the two brand names 3D Realms (formed in July 1994) and the now disused Pinball Wizards were created. Instead of publishing every game under Apogee as it had been in the past, the goal of this strategy was to create a different brand for each type of game genre, making each new game identifiable based on which brand it belonged to. This enabled Apogee to target different markets.
However, many of those varied genres such as platform or scrolling shooter (that were much of Apogee's early releases) were slowly dying out in the late 1990s, which made this strategy unnecessary. In addition, due to the increasingly lengthy development time in producing a game title, video game publishers were no longer releasing titles at the rapid rate at which they once were.
3D Realms was created in 1994 for the 3D game Terminal Velocity and was responsible for the latest installments of the successful Duke Nukem games and for producing the Max Payne series (earlier 3D games like Rise of the Triad were released under the Apogee name). The Pinball Wizards name was created for the 1997 pinball title Balls of Steel , but has not been used since.
The last game to be published under the Apogee name was Stargunner in 1996. Since 1998, all the company's games have been using a 3D engine (even if the gameplay is 2D, like in Duke Nukem: Manhattan Project ). As a result, 3D Realms has replaced Apogee as the brand name to publish games under. Also, by the end of the 1990s, Apogee felt their brand name was more associated with old, outdated games and adopted the 3D Realms brand for all future releases. [ citation needed ] When the 3D Realms name was first conceived, the official motto was Reality is our Game . This motto is no longer used.
The Apogee name was spun off as Apogee Software LLC in 2008, a separate company that would handle distribution, remakes, and other developments related to older Apogee games.

Restructuring
The latest game released by 3D Realms was Prey , on July 11, 2006 after being in development for eleven years. Prey was originally developed internally by 3D Realms, but after several years of delays, the company outsourced the development to Human Head Studios .
The other major project that 3D Realms was working on was Duke Nukem Forever , the sequel to Duke Nukem 3D . It was announced in 1997, and on May 6, 2009, its development was halted due to the development team being let go. [9] The official release date of the game was "when it's done." [10] During the years of the development of the game, some outside developers have developed and published Duke Nukem spin-offs.
On May 6, 2009, due to lack of funding, major staff cuts were initiated with the entire development team being laid off and other employees being given notice of their employment with the company being terminated. [11] The official company website briefly went offline on that day, but went back up soon afterwards. While there was no official statement at that moment on the closure, apart from messages on the 3D Realms forum, a final message appeared in the front page of the site, showing a group photo of the 3D Realms team, with the caption "Goodbye. Thanks for being fans and for all your support."
It was reported on May 14, 2009 that Take-Two , holders of the publishing rights of Duke Nukem Forever , filed a breach of contract suit against Apogee Software Ltd (3D Realms) over failing to deliver the aforementioned title. [12] Take-Two asked for a restraining order and a preliminary injunction, to make 3D Realms keep the Duke Nukem Forever assets intact during proceedings. [13] [14]
On May 18, 2009 3D Realms key executives released the first full official "press release" with their side of the developments. "... 3D Realms (3DR) has not closed and is not closing. ... Due to lack of funding, however, we are saddened to confirm that we let the Duke Nukem Forever (DNF) development team go on May 6,... While 3DR is a much smaller studio now, we will continue to operate as a company and continue to licence and co-create games based upon the Duke Nukem franchise. ... Take-Two’s proposal was unacceptable to 3DR for many reasons, including no upfront money, no guarantee minimum payment, and no guarantee to complete the DNF game. ...we viewed Take-Two as trying to acquire the Duke Nukem franchise in a “ fire sale .” ...we believe Take-Two’s lawsuit is without merit and merely a bully tactic to obtain ownership of the Duke Nukem franchise. We will vigorously defend ourselves against this publisher." [9]
On September 3, 2010, Take-Two announced that development of Duke Nukem Forever had been shifted over to Gearbox Software , effectively ending 3D Realms' association with the game after 12 years of stunted development. 3D Realms remained a co-developer on Duke Nukem Forever , due to their involvement in developing most of the game. However, the rights and intellectual property were sold to Gearbox, who are now the owners of the Duke Nukem franchise. [15] 3D Realms retained certain rights to the Duke Nukem back catalogue, but finally transferred all rights to Gearbox Software in 2015.
An external developer, Interceptor Entertainment , started work on a fan-project remake of Duke Nukem 3D in 2010. They received a limited authorization from Gearbox to proceed with the game, which was named Duke Nukem 3D: Reloaded . However, after Duke Nukem Forever' s release and negative reception in 2011, Duke Nukem 3D: Reloaded was put on hold indefinitely.
In an interview conducted with Scott Miller in April 2011, Miller specified that 3D Realms was involved with several projects citing, "Yes, we have several projects underway, all fairly small—not any big console games. Once DNF comes out we'll be definitely looking to invest into other projects, and maybe other up-n-coming teams who are blazing new trails on smaller platforms, like smart phones and XBLA. We have a long history of investing in young, unproven teams, going way back to Id Software, and including other notables like Parallax Software (we were the first studio to invest in Descent ), and Remedy Entertainment ( Death Rally and Max Payne ). So, we like that model and will keep doing it in the future. We seem to have a good eye for unproven talent waiting for some experienced guidance and hard-to-find funding". [16]

Current state
In June 2013, 3D Realms sued Gearbox for unpaid royalties as well as unpaid money for selling the Duke Nukem IP. [17] The lawsuit was dropped in September 2013 with 3D Realms apologizing with an announcement that they had resolved any differences they had with Gearbox.
3D Realms has since sold the rights of some of its older titles, leading to several remakes. One of them, Rise of the Triad , was developed by Interceptor Entertainment and published in 2013 by Apogee Software LLC . Another remake, Shadow Warrior , was developed by Flying Wild Hog , and published by Devolver Digital in 2013.
In February 2014, Gearbox sued 3D Realms, Interceptor Entertainment and Apogee Software for developing a new game called Duke Nukem: Mass Destruction . Gearbox stated that it was still the rights holder of the Duke Nukem franchise, and permission had not been granted by them to develop the game. 3D Realms soon after released a statement admitting its wrongdoing. [18] The lawsuit was settled in August 2015, with Gearbox stressing that it was still the lawful owner of the Duke Nukem IP. [19]
On March 3, 2014, it was announced that Danish investor Mike Nielsen's company SDN Invest (he is also behind Interceptor Entertainment ) bought 3D Realms and Nielsen was appointed as new CEO of the company. [20]
In May 2014, 3D Realms revealed they were working on a new game called Bombshell . [21] The game was released on 29 January 2016.

Games

As Apogee Software
Developer
Publisher/producer
Cancelled projects

As Pinball Wizards
Publisher/producer

As 3D Realms
Developer
Publisher/producer/licensor
Cancelled projects
WebPage index: 00005
Game engine
A game engine is a software framework designed for the creation and development of video games . Developers use them to create games for consoles , mobile devices and personal computers . The core functionality typically provided by a game engine includes a rendering engine (“renderer”) for 2D or 3D graphics , a physics engine or collision detection (and collision response), sound , scripting , animation , artificial intelligence , networking , streaming, memory management, threading, localization support, scene graph , and may include video support for cinematics. The process of game development is often economized, in large part, by reusing/adapting the same game engine to create different games, [1] or to make it easier to " port " games to multiple platforms. [ citation needed ]

Purpose
In many cases game engines provide a suite of visual development tools in addition to reusable software components. These tools are generally provided in an integrated development environment to enable simplified, rapid development of games in a data-driven manner. Game engine developers attempt to "pre-invent the wheel" by developing robust software suites which include many elements a game developer may need to build a game. Most game engine suites provide facilities that ease development, such as graphics, sound, physics and AI functions. These game engines are sometimes called " middleware " because, as with the business sense of the term, they provide a flexible and reusable software platform which provides all the core functionality needed, right out of the box, to develop a game application while reducing costs, complexities, and time-to-market — all critical factors in the highly competitive video game industry . [2] Gamebryo , JMonkey Engine and RenderWare are such widely used middleware programs. [3]
Like other middleware solutions, game engines usually provide platform abstraction, allowing the same game to be run on various platforms including game consoles and personal computers with few, if any, changes made to the game source code . Often, game engines are designed with a component-based architecture that allows specific systems in the engine to be replaced or extended with more specialized (and often more expensive) game middleware components such as Havok for physics, Miles Sound System for sound, or Bink for video. Some game engines such as RenderWare are even designed as a series of loosely connected game middleware components that can be selectively combined to create a custom engine, instead of the more common approach of extending or customizing a flexible integrated solution. However extensibility is achieved, it remains a high priority for game engines due to the wide variety of uses for which they are applied. Despite the specificity of the name, game engines are often used for other kinds of interactive applications with real-time graphical needs such as marketing demos, architectural visualizations, training simulations, and modeling environments. [4]
Some game engines only provide real-time 3D rendering capabilities instead of the wide range of functionality needed by games. These engines rely upon the game developer to implement the rest of this functionality or assemble it from other game middleware components. These types of engines are generally referred to as a "graphics engine," "rendering engine," or "3D engine" instead of the more encompassing term "game engine." This terminology is inconsistently used as many full-featured 3D game engines are referred to simply as "3D engines." A few examples of graphics engines are: Crystal Space , Genesis3D , Irrlicht , OGRE , RealmForge , Truevision3D , and Vision Engine . Modern game or graphics engines generally provide a scene graph , which is an object-oriented representation of the 3D game world which often simplifies game design and can be used for more efficient rendering of vast virtual worlds.
As technology ages, the components of an engine may become outdated or insufficient for the requirements of a given project. Since the complexity of programming an entirely new engine may result in unwanted delays (or necessitate that the project be completely restarted), a development team may elect to update their existing engine with newer functionality or components. [ citation needed ]

Components
Such a framework is composed of a multitude of very different components.

Main game program
The actual game logic has to be implemented by some algorithms . It is distinct from any rendering, sound or input work.

Rendering engine
The rendering engine generates 3D animated graphics by the chosen method ( rasterization , ray-tracing or any different technique).
Instead of being programmed and compiled to be executed on the CPU or GPU directly, most often rendering engines are built upon one or multiple rendering application programming interfaces (APIs), such as Direct3D or OpenGL which provide a software abstraction of the graphics processing unit (GPU).
Low-level libraries such as DirectX , Simple DirectMedia Layer (SDL), and OpenGL are also commonly used in games as they provide hardware-independent access to other computer hardware such as input devices (mouse, keyboard, and joystick), network cards, and sound cards. Before hardware-accelerated 3D graphics, software renderers had been used. Software rendering is still used in some modeling tools or for still-rendered images when visual accuracy is valued over real-time performance (frames-per-second) or when the computer hardware does not meet needs such as shader support.
With the advent of hardware accelerated physics processing, various physics APIs such as PAL and the physics extensions of COLLADA became available to provide a software abstraction of the physics processing unit of different middleware providers and console platforms.
Game engines can be written in any programming language like C++ , C or Java , though each language is structurally different and may provide different levels of access to specific functions.

Audio engine
The audio engine is the component which consists of algorithms related to sound. It can calculate things on the CPU, or on a dedicated ASIC. Abstraction APIs, such as OpenAL, SDL audio, XAudio 2, Web Audio, etc. are available.

Physics engine
The physics engine is responsible for emulating the laws of physics realistically within the application.

Artificial intelligence
The AI is usually outsourced from the main game program into a special module to be designed and written by software engineers with specialist knowledge.

History
Before game engines, games were typically written as singular entities: a game for the Atari 2600 , for example, had to be designed from the bottom up to make optimal use of the display hardware—this core display routine is today called the kernel by retro developers. Other platforms had more leeway, but even when the display was not a concern, memory constraints usually sabotaged attempts to create the data-heavy design that an engine needs. Even on more accommodating platforms, very little could be reused between games. The rapid advance of arcade hardware —which was the leading edge of the market at the time—meant that most of the code would have to be thrown out afterwards anyway, as later generations of games would use completely different game designs that took advantage of extra resources. Thus most game designs through the 1980s were designed through a hard-coded ruleset with a small number of levels and graphics data. Since the golden age of arcade video games , it became common for video game companies to develop in-house game engines for use with first party software .
While third-party game engines were not common up until the rise of 3D computer graphics in the 1990s, there were several 2D game creation systems produced in the 1980s for independent video game development . These include Pinball Construction Set (1983), ASCII 's War Game Construction Kit (1983), [5] Thunder Force Construction (1984), [6] Adventure Construction Set (1984), Garry Kitchen's GameMaker (1985), Wargame Construction Set (1986), Shoot'Em-Up Construction Kit (1987), Arcade Game Construction Kit (1988), and most popularly ASCII's RPG Maker engines from 1988 onwards. Klik & Play (1994) is another legacy offering that's still available.
The term "game engine" arose in the mid-1990s, especially in connection with 3D games such as first-person shooters (FPS). ( See also: first-person shooter engine .) Such was the popularity of Id Software 's Doom and Quake games that, rather than work from scratch, other developers licensed the core portions of the software and designed their own graphics, characters, weapons and levels —the "game content" or "game assets." Separation of game-specific rules and data from basic concepts like collision detection and game entity meant that teams could grow and specialize.
Later games, such as id Software 's Quake III Arena and Epic Games 's 1998 Unreal were designed with this approach in mind, with the engine and content developed separately. The practice of licensing such technology has proved to be a useful auxiliary revenue stream for some game developers, as a one license for a high-end commercial game engine can range from US$10,000 to millions of dollars, and the number of licensees can reach several dozen companies, as seen with the Unreal Engine . At the very least, reusable engines make developing game sequels faster and easier, which is a valuable advantage in the competitive video game industry . While there was a strong rivalry between Epic and id around 2000, since then Epic's Unreal Engine has been far more popular than id Tech 4 and its successor id Tech 5 . [7]
Modern game engines are some of the most complex applications written, often featuring dozens of finely tuned systems interacting to ensure a precisely controlled user experience. The continued evolution of game engines has created a strong separation between rendering, scripting, artwork, and level design . It is now common, for example, for a typical game development team to have several times as many artists as actual programmers. [8]
First-person shooter games remain the predominant users of third-party game engines, but they are now also being used in other genres . For example, the role-playing video game The Elder Scrolls III: Morrowind and the MMORPG Dark Age of Camelot are based on the Gamebryo engine, and the MMORPG Lineage II is based on the Unreal Engine. Game engines are used for games originally developed for home consoles as well; for example, the RenderWare engine is used in the Grand Theft Auto and Burnout franchises.
Threading is taking on more importance due to modern multi-core systems (e.g. Cell ) and increased demands in realism. Typical threads involve rendering, streaming, audio, and physics. Racing games have typically been at the forefront of threading with the physics engine running in a separate thread long before other core subsystems were moved, partly because rendering and related tasks need updating at only 30–60 Hz. For example, on PlayStation 3, physics ran in Need For Speed at 100 Hz versus Forza Motorsport 2 at 360 Hz.
Although the term was first used in the 1990s, there are a few earlier systems in the 1980s that are also considered to be game engines, such as Sierra's Adventure Game Interpreter (AGI) and SCI systems, LucasArts' SCUMM system and Incentive Software 's Freescape engine . Unlike most modern game engines, these game engines were never used in any third-party products (except for the SCUMM system which was licensed to and used by Humongous Entertainment ).

Recent trends
As game engine technology matures and becomes more user-friendly, the application of game engines has broadened in scope. They are now being used for serious games : visualization, training, medical, and military simulation applications, with the CryEngine being one example. [9] To facilitate this accessibility, new hardware platforms are now being targeted by game engines, including mobile phones (e.g. Android phones, iPhone ) and web browsers (e.g. WebGL , Shockwave , Flash , Trinigy 's WebVision, Silverlight , Unity Web Player , O3D and pure DHTML ). [10]
Additionally, more game engines are being built upon higher level languages such as Java and C# / .NET (e.g. TorqueX , and Visual3D.NET ), Python ( Panda3D ), or Lua Script (Leadwerks). As most 3D rich games are now mostly GPU -limited (i.e. limited by the power of the graphics card), the potential slowdown due to translation overheads of higher level languages becomes negligible, while the productivity gains offered by these languages work to the game engine developers' benefit. [11] These recent trends are being propelled by companies such as Microsoft to support Indie game development. Microsoft developed XNA as the SDK of choice for all video games released on Xbox and related products. This includes the Xbox Live Indie Games [12] channel designed specifically for smaller developers who don't have the extensive resources necessary to box games for sale on retail shelves. It is becoming easier and cheaper than ever to develop game engines for platforms that support managed frameworks. [13]

Game middleware
In the broader sense of the term, game engines themselves can be described as middleware. In the context of video games, however, the term "middleware" is often used to refer to subsystems of functionality within a game engine. Some game middleware does only one thing but does it more convincingly or more efficiently than general purpose middleware. For example, SpeedTree was used to render the realistic trees and vegetation in the role-playing video game The Elder Scrolls IV: Oblivion [14] and Fork Particle was used to simulate and render real time particle system visual effects or particle effects in Sid Meier's Civilization V . [15]
The four most widely used middleware packages [16] that provide subsystems of functionality include RAD Game Tools ' Bink, Firelight FMOD , Havok , and Scaleform GFx. RAD Game Tools develops Bink for basic video rendering, along with Miles audio, and Granny 3D rendering. Firelight FMOD is a low cost robust audio library and toolset. Havok provides a robust physics simulation system, along with a suite of animation and behavior solutions. Scaleform provides GFx for high performance Flash UI, along with a high quality video playback solution, and an Input Method Editor (IME) add-on for in-game Asian chat support.
Other middleware is used for performance optimisation - for example ' Simplygon ' helps to optimise and generate level of detail meshes, and ' Umbra ' adds occlusion culling optimisations to 3d graphics.
Some middleware contains full source code , others just provide an API reference for a compiled binary library . Some middleware programs can be licensed either way, usually for a higher fee for full source code.

Massively multiplayer online games
The Game Engine (or Middleware) for massively multiplayer online games (MMOs, MMOGs) is far more complex than for single-player video games. [ citation needed ] Technically every normal game engine can be used to implement an MMO game by combining it with MMO middleware. The increasing popularity of MMOGs is spurring development of MMO middleware packages. Some MMO middleware software packages already include a game engine, while others provide networking only and therefore must be combined with a game engine to create an MMO game.

First-person shooter engines
A well-known subset of game engines are 3D first-person shooter (FPS) game engines. Groundbreaking development in terms of visual quality is done in FPS games on the human scale. While flight and driving simulators and real-time strategy (RTS) games increasingly provide realism on a large scale, first-person shooters are at the forefront of computer graphics on these smaller scales.
The development of the FPS graphic engines that appear in games can be characterized by a steady increase in technologies, with some breakthroughs. Attempts at defining distinct generations lead to arbitrary choices of what constitutes a highly modified version of an "old engine" and what is a brand-new engine.
The classification is complicated as game engines blend old and new technologies. Features that were considered advanced in a new game one year become the expected standard the next year. Games with a mix of older generation and newer feature are the norm. For example, Jurassic Park: Trespasser (1998) introduced physics to the FPS games, but it did not become common until around 2002. Red Faction (2001) featured destructible walls and ground , something still not common in engines years later (for example in Unreal Tournament 2004 there are still no destructible objects). Battlezone (1998) and Battlezone II: Combat Commander (1999) added vehicle based combat to the usual FPS mix, which did not hit the mainstream until later. Tribes 2 , Battlefield 1942 , Halo: Combat Evolved , and Unreal Tournament 2004 fully realized the potential for vehicular-combat and first person shooter integration.

See also
WebPage index: 00006
William "B.J." Blazkowicz
William Joseph "B.J." Blazkowicz ( Polish pronunciation: [blasˈkɔvitʂ] ) is the protagonist of the Wolfenstein series of alternate history video games starting with 1992's Wolfenstein 3D . An American spy of Polish descent, he specializes in one-man missions behind enemy lines. In addition to fighting the regular German army he also frequently encounters bizarre Nazi experiments concerning biomechanical technology and the occult.

Biography
In the Wolfenstein series' backstory, Blazkowicz was born in the United States to a family of Polish immigrants . [3] During World War II , B.J. became a sergeant in the U.S. Army Rangers , before receiving his commanding officer's commission and being recruited as the top agent for the United States Office of Secret Actions (OSA), a fictional version of the Office of Strategic Services , who dispatched him to investigate rumors of occult activity by the Third Reich 's SS Paranormal Division (inspired by the real-world Ahnenerbe institute and the Thule Society ). After World War II, he eventually reared a son named Arthur, who came to have his own son who is the namesake of his grandfather. He would eventually star under a pseudonym , Billy Blaze, throughout the entire Commander Keen series. [4]

In video games
"B.J." Blazkowicz entered the Wolfenstein series with Wolfenstein 3D in 1992. The motion comic series created to promote 2009's Wolfenstein claims a continuous (partially retconned ) timeline with Wolfenstein 3D , [5] Spear of Destiny , [6] Return to Castle Wolfenstein , [7] and finally Wolfenstein [8] (later continued in Wolfenstein: The New Order ).
The missions that Blazkowicz participates in include assassinating a series of fictional leaders of German bio-chemical warfare research programs and eventually killing Adolf Hitler himself in Wolfenstein 3D , [5] defeating the Nazi plot to use the Spear of Destiny to summon the Angel of Death in Spear of Destiny , [6] and foiling Heinrich Himmler 's ritual to resurrect Heinrich I (a historical king from medieval German history, here portrayed by an evil necromancer) in Return to Castle Wolfenstein wherein he also finds out about Wilhelm "Deathshead" Strasse 's plan to create an army of undead cyborgs. [7]
In 2009's Wolfenstein , he returns to fight the resurgent Fourth Reich 's [8] use of a highly destructive energy of great power from the parallel world known as the Black Sun dimension, [9] which is again pitting him against Deathshead. In The New Order , Captain Blazkowicz suffers a head injury in 1946 that leaves him in a vegetative state for 14 years in a Polish asylum. In 1960, Blazkowicz awakens from his vegetative state as he is about to be executed, and joins the resistance against the Nazis who have conquered the whole world and who include his old nemesis Deathshead (Wilhelm Strasse). [10]
Wolfenstein RPG is set in an alternate timeline, which is mostly light-hearted and humorous, and serves as prequel to the Doom series. At the end, B.J. defeats and maims the Harbinger of Doom, a Nazi-summoned demon that is none other than the later Cyberdemon from Doom . Blazkowicz's descendant is Sergeant Stan Blazkowicz , the protagonist of Doom RPG and one of the protagonists of Doom II RPG . B.J. Blazkowicz himself was also supposed to star in an early sequel Rise of the Triad: Wolfenstein 3D Part II , which was eventually released as just Rise of the Triad with no Wolfenstein plot connection at all. [11]

Other appearances
In the 2005 German film Der Goldene Nazivampir von Absam 2 – Das Geheimnis von Schloß Kottlitz , [12] William "B.J." Blazkowicz, portrayed by Daniel Krauss, tracks down Nazi scientists in secret laboratories located in the Austrian Alps in order to disclose the secret of " miracle weapons " involving Dracula 's bones and to find that events occurring in the Kottlitz Castle are beyond imagination. [13] In 2007, Samuel Hadida bought the rights to make a more direct adaptation of the game series and Roger Avary was given task to write and direct the project that was said to be the story of B.J. Blazkowicz's mission to Hitler's Wolf's Lair . [14]
In May 2012, to celebrate the 20th anniversary of Wolfenstein 3-D , Bethesda Softworks released a free B.J. Xbox Live Avatar masks over on the series' Facebook page. [15] Classic B.J. Blazkowicz Mask was made a purchasable item for Doom at Xbox Live Marketplace . [16]

Reception
The character was well received. In 2008, IGN included B.J. Blazkowicz on the list of characters they would like to see in an ultimate fighting game , calling him "the soldier who fired the first shot in the first-person-shooter wars", [17] as well as in an ultimate "zombie strike team" of the best zombie fighters in entertainment. [18] IGN also listed him as first on a list of top commandos in video games, as "really, there's no greater victory for a commando than killing Hitler. Kudos, Blazkowicz." [19] In 2012, GamesRadar ranked him as 93rd "most memorable, influential, and badass" protagonist in video games, stating that "when you’ve single handedly defeat Mecha-Hitler, that pretty much makes you a hero in anybody’s book. B.J. has tirelessly slaughtered Nazis through three generations of hardware, and could be credited with kicking off the first-person shooter craze that led to games like Doom and Duke 3D ." [20]
In 2007, UGO.com included this "true American hero" on their list of the greatest soldiers in fantasy entertainment history, [21] also featuring him on the list of the greatest Jews in gaming: "Being the Nazi-hating son of Polish immigrants does make B.J. a candidate, but his Judaism remains woefully unconfirmed. For all the Nazis he's chain-gunned through, B.J. deserves a framed honorary Jew certificate." [22] The website Jew or Not Jew opined "he's probably just a Nazi-killing Pole." [23] Kotaku 's Stephen Totilo wrote that "the hints are" in The New Order that Blazkowicz is Jewish, such as his knowledge of written Hebrew . When Totilo contacted Bethesda Softworks , they told him it is "never explicitly stated" and the developer MachineGames decided to "leave it up to the player to interpret." [24]
According to Spike 's Jason Cipriano, " The New Order seems to be the first game that really gives the character some depth, and instead of just being another American, hell-bent on ending the Nazi regime Blazkowicz seems like he's a more multilayered character. B.J. has friends, a love interest, and a deeper reason to take down the Nazis: this time around he's not just trying to win a war - he's trying to save the world." [25] Anthony John Agnello of The A.V. Club noted MachineGames strived "to render Blazkowicz as a whole human being—at least, as human as he can be when he’s killing literally thousands of people, robots, dogs, and robot dogs." [26] GamesRadar 's Ryan Taljonick opined "B.J. [has become] a pretty interesting character, and delivers several internal monologues with just the right amount of drama" while Brian Bloom 's "fantastic voicework makes them believable." [27] In his review of The New Order , Lee Cooper of Hardcore Gamer wrote that "where it succeeds beyond the basest point is in its execution of characters, particularly Blazkowicz himself, who offers no more depth than porta potty but somehow manages to shine as leading man" and "the grittiest, manliest, most absolute Nazi-killin’ machine." [28]

See also
WebPage index: 00007
Shareware
Shareware is a type of proprietary software which is initially provided free of charge to users, who are allowed and encouraged to make and share copies of the program. Shareware is often offered as a download from an Internet website or as a compact disc included with a magazine. Shareware is available on all major personal computer platforms . The term shareware is used in contrast to open-source software , in which the source code is available for anyone to inspect and alter; and freeware , which is software distributed at no cost to the user but without source code being made available.
There are many types of shareware, and while they may not require an initial up-front payment, all are intended to generate revenue in one way or another. Some limit use to personal non- commercial purposes only, with purchase of a license required for use in a business enterprise. The software itself may be limited in functionality or be time-limited, or it may remind the user that payment would be appreciated.

Types of shareware

Adware
Adware, short for "advertising-supported software", is any software package which automatically renders advertisements in order to generate revenue for its author. The advertisements may be in the user interface of the software or on a screen presented to the user during the installation process. The functions may be designed to analyze which Internet sites the user visits and to present advertising pertinent to the types of goods or services featured there. The term is sometimes used to refer to software that displays unwanted advertisements.
On Microsoft Windows , shareware is often packaged with adware. [ citation needed ] During the install of the intended software, the user is presented with a requirement to agree to the terms of click through licensing or similar licensing which governs the installation of the software.

Demoware
Demoware is a demonstration version of software. There are generally two types demoware: that which is crippled, and that which has a trial period.

Crippleware
In software, crippleware means that vital features of the program such as printing or the ability to save files are disabled until the user buys the software. This allows users to take a close look at the features of a program without being able to use it to generate output. The distinction between freemium and crippleware is that an unlicensed freemium program has useful functionality, while crippleware demonstrates its potential but is not in itself useful.

Trialware
Trialware is software with a built-in time limit. The user can try out the fully featured program until the trial period is up, and then most trialware reverts to a reduced-functionality (freemium, nagware, or crippleware) or non-functional mode, unless the user pays the license fee and receives a registration code to unlock the program. Trialware has become the norm for online Software as a Service (SaaS) .
The rationale behind trialware is to give potential users the opportunity to try out the program to judge its usefulness before purchasing a license . According to industry research firm Softletter, 66% of online companies surveyed had free-trial-to-paying-customer conversion rates of 25% or less. [1] SaaS providers employ a wide range of strategies to nurture leads, and convert them into paying customers.

Donationware
Donationware is a licensing model that supplies fully operational unrestricted software to the user and requests an optional donation be paid to the programmer or a third-party beneficiary (usually a non-profit ). [2] The amount of the donation may also be stipulated by the author, or it may be left to the discretion of the user, based on individual perceptions of the software's value. Since donationware comes fully operational (i.e. not crippleware ) with payment optional, it is a type of freeware .

Nagware
Nagware (also known as begware , annoyware or a nagscreen ) is a pejorative term for shareware that persistently reminds the user to purchase a license. [3] It usually does this by popping up a message when the user starts the program, or intermittently while the user is using the application. These messages can appear as windows obscuring part of the screen, or as message boxes that can quickly be closed. Some nagware keeps the message up for a certain time period, forcing the user to wait to continue to use the program. Unlicensed programs that support printing may superimpose a watermark on the printed output, typically stating that the output was produced by an unlicensed copy.
Some titles display a dialog box with payment information and a message that paying will remove the notice, which is usually displayed either upon startup or after an interval while the application is running. These notices are designed to annoy the user into paying.

Freemium
Freemium works by offering a product or service free of charge (typically digital offerings such as software, content, games, web services or other) while charging a premium for advanced features, functionality, or related products and services. For example, a fully functional feature-limited version may be given away for free, with advanced features disabled until a license fee is paid. The word "freemium" is a portmanteau combining the two aspects of the business model: "free" and "premium". It has become a popular model. [ citation needed ]

History
In 1982, Andrew Fluegelman created a program for the IBM PC called PC-Talk , a telecommunications program, and used the term freeware ; he described it "as an experiment in economics more than altruism". [4] About the same time, Jim "Button" Knopf released PC-File , a database program, calling it user-supported software . [5] Not much later, Bob Wallace produced PC-Write , a word processor, and called it shareware . Appearing in an episode of Horizon titled Psychedelic Science originally broadcast 5 April 1998, Bob Wallace said the idea for shareware came to him "to some extent as a result of my psychedelic experience ". [6]
In 1983 Jerry Pournelle wrote of "an increasingly popular variant" of free software "that has no name, but works thus: "If you like this, send me (the author) some money. I prefer cash." [7] In 1984, Softalk-PC magazine had a column, The Public Library , about such software. Public domain is a misnomer for shareware, and Freeware was trademarked by Fluegelman and could not be used legally by others, and User-Supported Software was too cumbersome. So columnist Nelson Ford had a contest to come up with a better name.
The most popular name submitted was Shareware , which was being used by Wallace. However, Wallace acknowledged that he got the term from an InfoWorld magazine column by that name in the 1970s, and that he considered the name to be generic, [ citation needed ] so its use became established over freeware and user-supported software . [8]
Fluegelman, Knopf, and Wallace clearly established shareware as a viable software marketing method. Via the shareware model, Button, Fluegelman and Wallace became millionaires. [9] [10]
Prior to the popularity of the World Wide Web and widespread Internet access, Shareware was often the only economical way for independent software authors to get their product onto users' desktops. Those with Internet or BBS access could download software and distribute it amongst their friends or user groups, who would then be encouraged to send the registration fee to the author, usually via postal mail. During the late 1980s and early 1990s, shareware software was widely distributed over online services , bulletin board systems and on diskettes. Contrary to commercial developers who spent millions of dollars urging users " Don't Copy that Floppy ", shareware developers encouraged users to upload the software and share it on disks.
Commercial shareware distributors such as Educorp and Public Domain Inc printed catalogs describing thousands of public domain and shareware programs that were available for a small charge on floppy disk. These companies later made their entire catalog available on CD-ROM. One such distributor, Public Software Library (PSL), began an order-taking service for programmers who otherwise had no means of accepting credit card orders. Later, services like Kagi started offering applications that authors could distribute along with their products that would present the user with an onscreen form to fill out, print, and mail along with their payment. Once telecommunications became more widespread, this service also expanded online. Toward the beginning of the Internet era, books compiling reviews of available shareware were published, sometimes targeting specific niches such as small business . These books would typically come with one or more floppy disks or CD-ROMs containing software from the book. [11]
As Internet use grew, users turned to downloading shareware programs from FTP or web sites. This spelled the end of bulletin board systems and shareware disk distributors. At first, disk space on a server was hard to come by, so networks like Info-Mac were developed, consisting of non-profit mirror sites hosting large shareware libraries accessible via the web or ftp. With the advent of the commercial web hosting industry, the authors of shareware programs started their own sites where the public could learn about their programs and download the latest versions, and even pay for the software online. This erased one of the chief distinctions of shareware, as it was now most often downloaded from a central "official" location instead of being shared samizdat -style by its users. To ensure users would get the latest bug-fixes as well as an install untainted by viruses or other malware , some authors discouraged users from giving the software to their friends, encouraging them to send a link instead.
Major download sites such as VersionTracker and CNet 's Download.com began to rank titles based on quality, feedback, and downloads. Popular software was sorted to the top of the list, along with products whose authors paid for preferred placement.

Registration
If features are disabled in the freely accessible version, paying may provide the user with a licence key or code they can enter into the software to disable the notices and enable full functionality. Some pirate web sites publish license codes for popular shareware, leading to a kind of arms race between the developer and the pirates where the developer disables pirated codes and the pirates attempt to find or generate new ones. Some software publishers have started accepting known pirated codes, using the opportunity to educate users on the economics of the shareware model. [12]
Some shareware relies entirely on the user's honesty and requires no password. Simply checking an "I have paid" checkbox in the application is all that is required to disable the registration notices. [13] [14]

Games
In the early 1990s, shareware distribution was a popular method of publishing games for smaller developers, including then-fledgling companies Apogee Software (also known as 3D Realms ), Epic MegaGames (now Epic Games ), Ambrosia Software and id Software . It gave consumers the chance to play the game before investing money in it, and gave them exposure that some products would be unable to get in the retail space.
With the Kroz series, Apogee introduced the "episodic" shareware model that became the most popular incentive for buying a game. While the shareware game would be a truly complete game, there would be additional "episodes" of the game that were not shareware, and could only be legally obtained by paying for the shareware episode. In some cases these episodes were neatly integrated and would feel like a longer version of the game, and in other cases the later episodes would be stand-alone games. Sometimes the additional content was completely integrated with the unregistered game, such as in Ambrosia's Escape Velocity series, in which a character representing the developer's pet parrot , equipped with an undefeatable ship, would periodically harass and destroy the player after they reached a certain level representing the end of the trial period.
Racks of games on single 5 1/4 inch and later 3.5 inch floppy disks were common in retail stores. However, computer shows and bulletin board systems (BBS) such as Software Creations BBS were the primary distributors of low-cost software. Free software from a BBS was the motive force for consumers to purchase a computer equipped with a modem, so as to acquire software at no cost. [ citation needed ]
The important distinguishing feature between a shareware game and a game demo is that the shareware game is, at least in theory, a complete game. Where modern demos are often a single level or less, shareware games were usually complete games. Shareware episodes most commonly offered a significant fraction of the entire registered version, and many even offered the entire product as shareware with no additional content for registered users. [ citation needed ]

Criticism
Shareware has been characterized as being both less stable and less secure than officially developed software. [15]
In the 1980s and early-to-mid 1990s, shareware was a means of distribution enabling small developers to have their product widely disseminated among computer users, some of whom would purchase the product. After the internet became popular, the shareware model began to degrade [ citation needed ] as the term was used by commercial startups offering (sometimes substandard) commercial software and labeling non-functional or limited demo versions (termed crippleware ) as shareware. As a result, in the early 21st century, the term shareware was being used less, replaced by either demo or trial software. [ citation needed ]
Another type of shareware software distribution very popular in the mobile domain are app store markets. There, users can often obtain applications that are free and advert banner supported, and often a paid version with no ads and maybe more features.

Industry standards and technologies
There are several widely accepted standards and technologies that are used in the development and promotion of shareware.

See also
